{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a34a361c",
   "metadata": {},
   "source": [
    "# **Automatic Speach Recegnition (ASR) Model Selection**\n",
    "\n",
    "### In this notebook, I shall apply precise and educated tests - written based on our project's desired objectives and standards - on a list of the top ASR models out there, to nominate the best of them to us. And teh best performance wasn't good enough, then we will need to fine-tune the models and test them again in another notebook.\n",
    "\n",
    "### But before doing so, I will research and dive into each model in a sufficient manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb544af",
   "metadata": {},
   "source": [
    "## **0Ô∏è‚É£ Table Of Contents**\n",
    "1. [Standards](#standards)  \n",
    "2. [ASR Models](#asr-models)  \n",
    "    2.1 [Whisper (OpenAI)](#asr-models-whisper)  \n",
    "    2.2 [Wav2Vec‚ÄØ2.0 (Meta)](#asr-models-wav2vec)  \n",
    "    2.3 [Parakeet TDT 0.6B v2 (Nvidia)](#asr-models-parakeet)  \n",
    "    2.4 [Canary 1B (Nvidia)](#asr-models-canary)  \n",
    "3. [Datasets](#datasets)  \n",
    "    3.1 [Common Voice](#datasets-common-voice)  \n",
    "    3.2 [Librispeech](#datasets-librispeech)  \n",
    "    3.3 [TED-LIUM](#datasets-tedlium)  \n",
    "    3.4 [MathSpeech](#datasets-mathspeech)  \n",
    "4. [Preprocessing](#preprocessing)  \n",
    "5. [Evaluation Metrics](#evaluation-metrics)  \n",
    "6. [Preparing The Testing Codes](#preparing-the-testing-codes)  \n",
    "    6.1 [Login To Hugging Face](#preparing-the-testing-codes-login-to-hugging-face)  \n",
    "    6.2 [Create Classes For The ASR Models](#preparing-the-testing-codes-create-classes-for-the-asr-models)  \n",
    "    6.3 [Dataset Class](#preparing-the-testing-codes-datasets-class)  \n",
    "    6.4 [Preprocessors](#preparing-the-testing-codes-preprocessors)  \n",
    "    6.5 [Testing Loop](#preparing-the-testing-codes-testing-loop)  \n",
    "    6.6 [Start The Tests](#preparing-the-testing-codes-start-the-tests)  \n",
    "    6.7 [Important Notes](#preparing-the-testing-codes-important-notes)  \n",
    "7. [Results](#results)  \n",
    "8. [Final Words](#final-words)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f7a96",
   "metadata": {},
   "source": [
    "## **1Ô∏è‚É£ Standards** <a id=\"standards\"></a>\n",
    "\n",
    "##### <font color='#D55'>Note: The following standards are not final, and they are open to discuss and change</font>\n",
    "\n",
    "### The following are the standards that models will be competing for:\n",
    "- #### Robust to noise\n",
    "- #### Immune to accents (even poor ones)\n",
    "- #### High-accuracy transcription\n",
    "- #### Endures specialized topics and irregular language, such as mathematical expressions, codes, etc...\n",
    "- #### Not expinsive for us\n",
    "- #### Multilingual<font color='#080'># This is ignored for now</font>\n",
    "- #### fine-tunable<font color='#080'># This one will be ignored if the models performed good enough</font>\n",
    "\n",
    "### All models will be tested based on the previous standards as show in the \"Models Evaluation\" section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45218f17",
   "metadata": {},
   "source": [
    "## **2Ô∏è‚É£ ASR Models** <a id=\"asr-models\"></a>\n",
    "\n",
    "### This section provides a list of the most famous ASR models each with a detailed table and a brief description\n",
    "\n",
    "#### The nominated models must be open-weight and have competitive performance. All old and poor ASR got neglected.\n",
    "\n",
    "### 1. Whisper (OpenAI) <a id=\"asr-models-whisper\"></a>\n",
    "> Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.\n",
    "\n",
    "| Feature | Details |\n",
    "|----------|----------|\n",
    "| License  | MIT |\n",
    "| Weights  | open-weights, <font color='#D55'>need other forks for fine-tuning and<br> it is resource-intinsive</font> |\n",
    "| Strangths  | High-accuracy transcription, multilingual, translator,<br> robust to accents, good with non-English, trained on noise,<br> supports timestamps |\n",
    "| Weaknesses  | not fine-tuned on special topics |\n",
    "| Parameters  | 39 M, 74 M, 244 M, 769 M, 809 M, 1550 M |\n",
    "\n",
    "### Sizes\n",
    "\n",
    "| Size | Parameters | Relative speed |\n",
    "|----------|:----------:|:----------:|\n",
    "| tiny | 39 M | ~10x |\n",
    "| base | 74 M | ~7x |\n",
    "| small | 244 M | ~4x |\n",
    "| medium | 769 M | ~2x |\n",
    "| turbo | 809 M | ~8x |\n",
    "| large | 1550 M | 1x |\n",
    "\n",
    "### References: \n",
    "- https://openai.com/index/whisper/\n",
    "- https://github.com/openai/whisper?tab=readme-ov-file\n",
    "- https://huggingface.co/blog/fine-tune-whisper#closing-remarks\n",
    "\n",
    "### 2. Wav2Vec‚ÄØ2.0 (Meta) <a id=\"asr-models-wav2vec\"></a>\n",
    "> A Framework for Self-Supervised Learning of Speech Representations.\n",
    "\n",
    "> We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler.\n",
    "\n",
    "| Feature | Details |\n",
    "|----------|----------|\n",
    "| License  | MIT |\n",
    "| Weights  | open-weights, fine-tunable |\n",
    "| Strangths  | requires little to no transcribed data (self-supervision), so it<br> can learn on unlabled data |\n",
    "| Weaknesses  | Originally English-only |\n",
    "| Parameters  | 95 M, 317 M |\n",
    "\n",
    "### Variations\n",
    "\n",
    "| Model | Parameters | Training | Link |\n",
    "|----------|:----------:|----------|:----------:|\n",
    "| facebook/wav2vec2-base | 95 M | Pretrained on unlabedled data, no fine-tuning. | [Link](https://huggingface.co/facebook/wav2vec2-base) |\n",
    "| facebook/wav2vec2-base-960h | 95 M | Fine-tuned wav2vec2-base on 960 hours LibriSpeech | [Link](https://huggingface.co/facebook/wav2vec2-base-960h) |\n",
    "| facebook/wav2vec2-large | 317 M | Pretrained on unlabedled data, no fine-tuning. | [Link](https://huggingface.co/facebook/wav2vec2-large) |\n",
    "| facebook/wav2vec2-large-960h | 317 M | Fine-tuned wav2vec2-base on 960 hours LibriSpeech | [Link](https://huggingface.co/facebook/wav2vec2-large-960h) |\n",
    "| facebook/wav2vec2-large-960h-lv60-self | 317 M | Self-trained on Libri-Light 60k hours + fine-tuned on 960h | [Link](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self) |\n",
    "| facebook/wav2vec2-large-xlsr-53 | 317 M | Trained on 56k hours of multilingual unlabeled audio (53 languages),<br>no fine-tuning | [Link](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) |\n",
    "\n",
    "### References: \n",
    "- https://ai.meta.com/research/impact/wav2vec/\n",
    "- https://arxiv.org/abs/2006.11477\n",
    "- https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec#wav2vec-20\n",
    "\n",
    "### 3. Parakeet TDT (Nvidia) <a id=\"asr-models-parakeet\"></a>\n",
    "> An automatic speech recognition (ASR) model designed for high-quality English transcription, featuring support for punctuation, capitalization, and accurate timestamp prediction.\n",
    "\n",
    "| Feature | Details |\n",
    "|----------|----------|\n",
    "| License  | CC‚ÄëBY‚Äë4.0 |\n",
    "| Weights  | open-weights |\n",
    "| Strangths  | Efficiently transcribes long audio segments, Robust<br> performance on spoken numbers and song lyrics transcription,<br> robust to noise |\n",
    "| Weaknesses  | English-only |\n",
    "| Parameters  | 600 M, 1.1 B |\n",
    "\n",
    "### Variations\n",
    "\n",
    "| Model | Parameters | Link |\n",
    "|----------|:----------:|:----------:|\n",
    "| nvidia/parakeet-tdt-0.6b-v2 | 0.6 B | [Link](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2) |\n",
    "| nvidia/parakeet-tdt-1.1b | 1.1 B | [Link](https://huggingface.co/nvidia/parakeet-tdt-1.1b) |\n",
    "| nvidia/parakeet-rnnt-1.1b | 1.1 B | [Link](https://huggingface.co/nvidia/parakeet-rnnt-1.1b) |\n",
    "| nvidia/parakeet-ctc-0.6b | 0.6 B | [Link](https://huggingface.co/nvidia/parakeet-ctc-0.6b) |\n",
    "| nvidia/parakeet-ctc-1.1b | 1.1 B | [Link](https://huggingface.co/nvidia/parakeet-ctc-1.1b) |\n",
    "\n",
    "\n",
    "### References: \n",
    "- https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2\n",
    "- https://huggingface.co/nvidia/parakeet-tdt-1.1b\n",
    "- https://huggingface.co/nvidia/parakeet-rnnt-1.1b\n",
    "- https://huggingface.co/nvidia/parakeet-ctc-0.6b\n",
    "- https://huggingface.co/nvidia/parakeet-ctc-1.1b\n",
    "\n",
    "### 4. Canary (Nvidia) <a id=\"asr-models-canary\"></a>\n",
    "\n",
    "### <font color='#D55'>This one will just be ignored for now. If the other models didn't perform good enough, then I will try to use it again. But I really tried hard and wasted too much valuable time with infinite compatipilities issues with all the APIs I tried</font>\n",
    "\n",
    "> A multilingual model that transcribes speech in English, Spanish, German, and French with punctuation and capitalization. Canary also provides bi-directional translation, between English and the three other supported languages.\n",
    "\n",
    "| Feature | Details |\n",
    "|----------|----------|\n",
    "| License  | CC BY-NC 4.0 |\n",
    "| Weights  | open-weights |\n",
    "| Strangths  | 4 languages, translator, High accuracy |\n",
    "| Weaknesses  | GPU-heavy |\n",
    "| Parameters  | 883 M, 1 B |\n",
    "\n",
    "### Variations\n",
    "\n",
    "| Model | Parameters | Link |\n",
    "|----------|:----------:|:----------:|\n",
    "| nnvidia/canary-1b | 1 B | [Link](https://huggingface.co/nvidia/canary-1b) |\n",
    "| nvidia/canary-1b-flash | 883 M | [Link](https://huggingface.co/nvidia/canary-1b-flash) |\n",
    "\n",
    "### References: \n",
    "- https://developer.nvidia.com/blog/new-standard-for-speech-recognition-and-translation-from-the-nvidia-nemo-canary-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d163b1",
   "metadata": {},
   "source": [
    "## **3Ô∏è‚É£ Datasets** <a id=\"datasets\"></a>\n",
    "\n",
    "### These are the datasets that I am going to test the models on. The datasets are made to be small because this is only a test, and no need for wasting much time.\n",
    "\n",
    "### **1. Common Voice** <a id=\"datasets-common-voice\"></a>\n",
    "\n",
    "The Common Voice dataset is an open-source collection of voice recordings contributed by volunteers from around the world. Created by Mozilla, it includes transcribed speech data in dozens of languages and accents, making it one of the largest multilingual datasets available for training automatic speech recognition (ASR) systems. The dataset is freely available under the CC-0 license, encouraging both academic and commercial use. It aims to support inclusive and diverse voice technology by representing different accents, demographics, and speaking styles.\n",
    "\n",
    "The dataset I will be using is a subset of the Common Voice 13.0 dataset that I created. The subset only contains the testing data, and just in English, shrinking the data size significantly. Subset contains 16,372 samples, with a total size of 672 MB. Since currently I am only using the data for testing, this will help speeding up the process and minimize the needed resources. For now I am only want some data to test the models on, thus, no need for big data.\n",
    "\n",
    "The subset can be found here: https://huggingface.co/datasets/BinAlsadiq/common_voice_13_0_en_test\n",
    "\n",
    "The original dataset: https://commonvoice.mozilla.org/en/datasets\n",
    "\n",
    "### **Features & Characteristics**\n",
    "\n",
    "- **Sourced from**<br>\n",
    "    Volunteers are free to contibute by recording themselves reading public domain (CC0) sentences in their languages and accents. The public domain (CC0) sentences typically sourced from:\n",
    "    - Wikipedia\n",
    "    - Public domain books\n",
    "    - Government websites\n",
    "    - News articles\n",
    "    - User-submitted content\n",
    "    - Other public domain sources\n",
    "    \n",
    "- **Typical Topics**<br>\n",
    "    Technology, Science, Weather, History, Education, Geography, Daily Life, Health, Transportation, etc.\n",
    "- **Contains huge, and an unbalanced variety of accents and supports a lot of languages**<br>\n",
    "    - Supports 137 different language\n",
    "    - A single sample can have multiple accents because speakers self-report their accent(s), and the system allows tagging multiple ‚Äî even if not all are equally reflected in each recording.\n",
    "    - Illustrative Example: A bilingual speaker raised in different regions might reflect a mix.\n",
    "- **Noise**<br>\n",
    "    The Common Voice dataset intentionally includes a variety of real-world background noises to make automatic speech recognition (ASR) more robust.<br>\n",
    "    \n",
    "    The accepted noises might include:\n",
    "    - Traffic noise\n",
    "    - Indoor chatter or distant conversations\n",
    "    - Household sounds (TV, cooking, machinery)\n",
    "    - Quiet background music\n",
    "    - Natural ambient sounds (birds, wind, etc.)\n",
    "\n",
    "    The unaccepted noises might include:\n",
    "    - Excessive loud background noise\n",
    "    - Overlapping conversations interfering with the clip\n",
    "    - Cracking, distortion, static, or dropouts that obscure speech \n",
    "- **Censorship**<br>\n",
    "    - Sentences are reviewed and curated\n",
    "    - No hate speech, obscenity, or private data is allowed\n",
    "\n",
    "- **Average Clip Duration**<br>\n",
    "    The average clip duration is approximatly 5s.\n",
    "\n",
    "### **Some Issues To Consider**\n",
    "- **capitalization:** Not all sentences contain correct capitalization. For example:<br>\n",
    "    - A sentence starts with a capital letter in: common_voice_en_18276812.mp3 : \"What a strange mauve colour!\"\n",
    "    - But here it doesn't: common_voice_en_93276.mp3 : \"a boy enjoys a rain shower.\"\n",
    "\n",
    "    More serious example where all leters are written in capitals: <br>\n",
    "    - common_voice_en_572372.mp3 : \"YOU WANNA TAKE THIS OUTSIDE?\"\n",
    "\n",
    "These issues are serious, and will affect the models predictions. Thus, I will need to deal with them as will be shown later.\n",
    "\n",
    "### **Missing Features**\n",
    "The dataset doesn't contain irregular language such as mathematical expressions.\n",
    "\n",
    "### **Dataset Structure & Details**<br>\n",
    "Everything I mention here is about my version of the dataset, and it might be slightly different from the original.\n",
    "\n",
    "The dataset contains 16,372 samples, each sample contains the following values:\n",
    "- **path:** contains the audio file name correspnding to this sample. For example, common_voice_en_57.mp3\n",
    "- **audio:** This is a dictionary contains three values:\n",
    "    - **path:** The absolute path to the corresponding audio file\n",
    "    - **array:** An array of floats representing the loadede corresponding audio file\n",
    "    - **sampling_rate:** The sampling rate of the corresponding audio file\n",
    "- **sentence:** Transcription\n",
    "- **up_votes:** Total number of up votes made by reviewers\n",
    "- **down_votes:** Total number of down votes made by reviewers\n",
    "- **age:** Contributer's age\n",
    "- **gender:** Contributer's gender\n",
    "- **accents:** Contributer's accents<br>\n",
    "    As previously mentioned:\n",
    "    - A single sample can have multiple accents because speakers self-report their accent(s), and the system allows tagging multiple ‚Äî even if not all are equally reflected in each recording\n",
    "    - Illustrative Example: A bilingual speaker raised in different regions might reflect a mix\n",
    "- **locale:** The language code of the audio sample, such as en for English, and et for Estonian\n",
    "- **segment:** marks whether a clip belongs to a specific special-purpose sub-corpus beyond the main dataset\n",
    "\n",
    "The following values, path, audio, sentence, up_votes, and down_votes always have values, while the rest might be empty sometimes.\n",
    "\n",
    "Reviewers can check the samples and vote for each one, either up, indicating that the transcription is correct, or down, indicating that the transcription is incorrect or the clip has poor quality. All the samples in this dataset had more up votes than the down votes at least by a value of 1.\n",
    "\n",
    "### **References**\n",
    "https://github.com/common-voice/sentence-collector<br>\n",
    "https://www.mozillafoundation.org/en/blog/the-new-common-voice-sentence-collector/<br>\n",
    "https://www.mozillafoundation.org/en/blog/guidance-for-splinter-datasets-on-mcv/<br>\n",
    "\n",
    "### **Exploring The Dataset**\n",
    "\n",
    "- #### **Download It**\n",
    "```Python\n",
    "from datasets import load_dataset\n",
    "\n",
    "# download the dataset from hugging face\n",
    "dataset = load_dataset(\"BinAlsadiq/common_voice_13_0_en_test\", split='test')\n",
    "dataset\n",
    "```\n",
    "**output:**\n",
    "```Python\n",
    "Dataset({\n",
    "    features: ['path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accents', 'locale', 'segment'],\n",
    "    num_rows: 16372\n",
    "})\n",
    "```\n",
    "\n",
    "- #### **Dataset Format**\n",
    "```Python\n",
    "print(f\"path: {dataset['path'][0]}\\n\")\n",
    "print(f\"audio: {dataset['audio'][0]}\\n\")\n",
    "print(f\"sentence: {dataset['sentence'][0]}\\n\")\n",
    "print(f\"up_votes: {dataset['up_votes'][0]}\\n\")\n",
    "print(f\"down_votes: {dataset['down_votes'][0]}\\n\")\n",
    "print(f\"age: {dataset['age'][0]}\\n\")\n",
    "print(f\"gender: {dataset['gender'][0]}\\n\")\n",
    "print(f\"accents: {dataset['accents'][0]}\\n\")\n",
    "print(f\"locale: {dataset['locale'][0]}\\n\")\n",
    "print(f\"segment: {dataset['segment'][0]}\")\n",
    "```\n",
    "**output:**\n",
    "```Python\n",
    "path: common_voice_en_27710027.mp3\n",
    "\n",
    "audio: {'path': '/root/.cache/huggingface/datasets/downloads/extracted/d79bd49d7b0d227879d17d015d61a097f0548d853ad4253867049733836db92d/data/clips/common_voice_en_27710027.mp3', 'array': array([-2.09547579e-09, -1.39698386e-09, -1.86264515e-09, ...,\n",
    "        7.56020654e-07,  8.43494490e-07,  1.62231663e-06]), 'sampling_rate': 16000}\n",
    "\n",
    "sentence: Joe Keaton disapproved of films, and Buster also had reservations about the medium.\n",
    "\n",
    "up_votes: 3\n",
    "\n",
    "down_votes: 1\n",
    "\n",
    "age: \n",
    "\n",
    "gender: \n",
    "\n",
    "accents: \n",
    "\n",
    "locale: en\n",
    "\n",
    "segment: \n",
    "```\n",
    "\n",
    "- #### **Accents Variety & Distribution**\n",
    "How many, and what accents are there:\n",
    "```Python\n",
    "print(len(df['accents'].unique()))\n",
    "print(df['accents'].unique())\n",
    "```\n",
    "**output:**\n",
    "```Python\n",
    "93\n",
    "['' 'United States English,American Midwestern' 'Hong Kong English'\n",
    " 'Filipino' 'United States English' 'United States English,wolof'\n",
    " 'England English' 'Australian English'\n",
    " 'Southern African (South Africa, Zimbabwe, Namibia)'\n",
    " 'India and South Asia (India, Pakistan, Sri Lanka)'\n",
    " 'United States English,Puerto Rican,Latin American English,Florida,New York,Long Island,Savannah, Georgia'\n",
    " 'Canadian English' 'Polish English,England English' 'Scottish English'\n",
    " 'England English,Swedish' 'German English,\"denglish\"'\n",
    " 'United States English,Hong Kong English'\n",
    " 'Upstate New York,United States English' 'Singaporean English' 'Russian'\n",
    " 'Chichester' 'United States English,Gay' 'Welsh English'\n",
    " 'West Indies and Bermuda (Bahamas, Bermuda, Jamaica, Trinidad)'\n",
    " 'Non native' 'Deutsch English'\n",
    " 'I think mine accent is influenced by Indian Accent ,Yes Please. ,India and South Asia (India, Pakistan, Sri Lanka)'\n",
    " 'Irish English' 'Malaysian English' 'Singaporean English,Thai English'\n",
    " 'England English,United States English'\n",
    " 'United States English,Southern African (South Africa, Zimbabwe, Namibia)'\n",
    " 'United States English,West Coast' 'Brooklyn '\n",
    " 'England English,Midlands English'\n",
    " 'United States English,Southern New England English (Boston, Worcester, Lowell Area)'\n",
    " 'United States English,England English'\n",
    " 'India and South Asia (India, Pakistan, Sri Lanka),United States English'\n",
    " 'English (Native Greek speaker)' 'United States English,Dutch' 'Catalan'\n",
    " 'indian' 'Irish English,English' 'United States English,Midwestern'\n",
    " 'United States English,Filipino' 'United States English,Californian'\n",
    " 'United States English,Mid Atlantica ,African American Vernacular '\n",
    " 'french accent'\n",
    " 'United States English,Born and lived in eastern VA for 8 years. Then lived in southern CA for 13 years.  Lived in MD, NC, WA, HI  for 1-3 years each.  Spent 30 years in Washington DC area and 17 years in Northern KY/Cincinnati OH area'\n",
    " 'United States English,Northeast US' 'I have none that I can tell.'\n",
    " 'Russian English' 'Scottish English,Glaswegian '\n",
    " 'England English,Bedford English,Cambridge English'\n",
    " 'United States English,India and South Asia (India, Pakistan, Sri Lanka)'\n",
    " 'United States English,United States-West Coast-Alaska,United States-Midwestern'\n",
    " 'Neutral,indian,slow' 'England English,Hong Kong English'\n",
    " 'New Zealand English' 'Argentinian English'\n",
    " 'Canadian English,Welsh English' 'Filipino,Bisaya' 'french english'\n",
    " 'Haitian Creole' 'French'\n",
    " \"New Zealand English,I don't really speak english, just practicing\"\n",
    " 'United States English,Canadian English,Indo-Canadian English'\n",
    " 'English (UK)' 'United States English,Slight Dutch accent'\n",
    " 'United States English,Chicago ,Midwestern,Gen Z'\n",
    " 'United States English,Scandinavian' 'Western Europe'\n",
    " 'United States English,Scottish English,Irish English,England English'\n",
    " 'United States English,England English,Hong Kong English'\n",
    " 'Southern African (South Africa, Zimbabwe, Namibia),Durban'\n",
    " 'United States English,English Second Language'\n",
    " 'United States English,country'\n",
    " 'slightly slurred due to age and alcohol consumption.'\n",
    " 'Canadian English,United States English' 'South Australia'\n",
    " 'northern cali' 'England English,Received Pronunciation' 'Polish English'\n",
    " 'I was born in England and have lived in Australia, Canada and France.'\n",
    " \"Israeli's accent \"\n",
    " 'England English,Southern African (South Africa, Zimbabwe, Namibia)'\n",
    " 'United States English,Australian English' 'Second tongue'\n",
    " 'Australian English,New Zealand English' 'Italian,England English'\n",
    " 'very slight Russian accent,Standard American English,Boston influence'\n",
    " 'Southern Texas Accent,United States English' 'serbian']\n",
    "\n",
    "```\n",
    "\n",
    "How many samples don't have a specified accent value:\n",
    "```Python\n",
    "len(df[df['accents'] == ''])\n",
    "```\n",
    "**output:**\n",
    "```Python\n",
    "14822\n",
    "```\n",
    "\n",
    "There are 14822 samples out of the total 16372 that don't have values for the accents column. But, let's see the distribution of the top 10 accents of the other samples:\n",
    "```Python\n",
    "df[df['accents'] != '']['accents'].value_counts(normalize=True)[0:10]\n",
    "```\n",
    "**output:**\n",
    "```Python\n",
    "accents\n",
    "United States English                                 0.434839\n",
    "India and South Asia (India, Pakistan, Sri Lanka)     0.226452\n",
    "England English                                       0.119355\n",
    "Canadian English                                      0.042581\n",
    "Australian English                                    0.019355\n",
    "Southern African (South Africa, Zimbabwe, Namibia)    0.016129\n",
    "Hong Kong English                                     0.010968\n",
    "Irish English                                         0.009677\n",
    "New Zealand English                                   0.008387\n",
    "Scottish English                                      0.007097\n",
    "Name: proportion, dtype: float64\n",
    "```\n",
    "\n",
    "### **2. Librispeech** <a id=\"datasets-librispeech\"></a>\n",
    "\n",
    "> LibriSpeech is a corpus of approximately 1000 hours of 16kHz read English speech, prepared by Vassil Panayotov with the assistance of Daniel Povey. The data is derived from read audiobooks from the LibriVox project, and has been carefully segmented and aligned.\n",
    "\n",
    "The dataset can be fount at: https://www.openslr.org/12 or https://huggingface.co/datasets/openslr/librispeech_asr\n",
    "\n",
    "Note, I will use both the \"clean\" and \"other\" data as will be shown later on. Continue reading to learn waht the \"clean\" and \"other\" are.\n",
    "\n",
    "### **Features & Characteristics**\n",
    "- **Sourced from**<br>\n",
    "    The data is derived from read audiobooks from the [LibriVox](https://librivox.org/) project, and has been carefully segmented and aligned.\n",
    "- **Typical Topics**<br>\n",
    "    Since it is sourced out of books, it contain a wide variety of topics including classical literature, religions, science, etc.\n",
    "- **Accents & Languages**<br>\n",
    "    - Only English.\n",
    "    - Supports different English accents, but it is mainly consists of the American accent.\n",
    "        - There is no official labeling of accent type in the metadata\n",
    "- **Noise**<br>\n",
    "    The samples that are tagged as clean don't contain much noise. While the other samples might contain echo, background noises, poor microphone quality, etc.\n",
    "- **Average Clip Duration**<br>\n",
    "    The average clip duration is approximatly 12.5s.\n",
    "\n",
    "### **Some Issues To Consider**\n",
    "The texts are normalized, this led to:\n",
    "- All letters are in capital\n",
    "- No punctuations\n",
    "\n",
    "### **Missing Features**\n",
    "The dataset doesn't contain irregular language such as mathematical expressions.\n",
    "\n",
    "### **Dataset Structure & Details**<br>\n",
    "Each sample contains the following values:\n",
    "- **chapter_id:** A unique ID for the chapter that is being read from\n",
    "- **file:** The absolute path file in .flac format\n",
    "- **audio:** This is a dictionary contains three values:\n",
    "    - **file:** The absolute path file in .flac format\n",
    "    - **array:** An array of floats representing the loadede corresponding audio file\n",
    "    - **sampling_rate:** The sampling rate of the corresponding audio file\n",
    "- **id:** A unique ID for the sample\n",
    "- **speaker_id:** A unique ID for the speaker\n",
    "- **text:** Transcription\n",
    "\n",
    "The data samples are split as follows:\n",
    "|  | Train.500 | Train.360 | Train.100 | Valid | Test |\n",
    "|----------|:----------:|:----------:|:----------:|:----------:|:----------:|\n",
    "| clean | - | 104014 | 28539 | 2703 | 2620 |\n",
    "| other | 148688 | - | - | 2864 | 2939 |\n",
    "\n",
    "The 500, 360, and 100 represents the total hours.\n",
    "\n",
    "> The audio is in English. There are two configurations: clean and other. The speakers in the corpus were ranked according to the WER of the transcripts of a model trained on a different dataset, and were divided roughly in the middle, with the lower-WER speakers designated as \"clean\" and the higher WER speakers designated as \"other\".\n",
    "\n",
    "### **References**\n",
    "https://www.openslr.org/12<br>\n",
    "https://ieeexplore.ieee.org/document/7178964<br>\n",
    "https://huggingface.co/datasets/openslr/librispeech_asr<br>\n",
    "\n",
    "### **Exploring The Dataset**\n",
    "\n",
    "- #### **Download It**\n",
    "```Python\n",
    "from datasets import load_dataset\n",
    "\n",
    "# download the dataset from hugging face\n",
    "dataset = load_dataset(\"BinAlsadiq/librispeech_test_clean\", split='test')\n",
    "dataset\n",
    "```\n",
    "**output:**\n",
    "```Python\n",
    "Dataset({\n",
    "    features: ['chapter_id', 'file', 'audio', 'id', 'speaker_id', 'text'],\n",
    "    num_rows: 2620\n",
    "})\n",
    "```\n",
    "\n",
    "- #### **Dataset Format**\n",
    "```Python\n",
    "print(f\"chapter_id: {dataset['chapter_id'][0]}\\n\")\n",
    "print(f\"file: {dataset['file'][0]}\\n\")\n",
    "print(f\"audio: {dataset['audio'][0]}\\n\")\n",
    "print(f\"id: {dataset['id'][0]}\\n\")\n",
    "print(f\"speaker_id: {dataset['speaker_id'][0]}\\n\")\n",
    "print(f\"text: {dataset['text'][0]}\\n\")\n",
    "```\n",
    "**output:**\n",
    "```Python\n",
    "chapter_id: 123286\n",
    "\n",
    "file: /root/.cache/huggingface/datasets/downloads/extracted/f26361781344d4881219a6e350ef1f761e4945e8b4c0e7118ec8800a7cfc9351/data/260/123286/260-123286-0000.flac\n",
    "\n",
    "audio: {'path': '/root/.cache/huggingface/datasets/downloads/extracted/f26361781344d4881219a6e350ef1f761e4945e8b4c0e7118ec8800a7cfc9351/data/260/123286/260-123286-0000.flac', 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
    "       -9.15527344e-05, -9.15527344e-05, -9.15527344e-05]), 'sampling_rate': 16000}\n",
    "\n",
    "id: 0\n",
    "\n",
    "speaker_id: 260\n",
    "\n",
    "text: SATURDAY AUGUST FIFTEENTH THE SEA UNBROKEN ALL ROUND NO LAND IN SIGHT\n",
    "```\n",
    "\n",
    "### **3. TED-LIUM** <a id=\"datasets-tedlium\"></a>\n",
    "\n",
    "The TED-LIUM dataset is a large-scale corpus of English-language speech collected from TED Talks, created by the LIUM Spoken Language Processing Group. It includes professionally recorded audio files, aligned transcripts, and speaker metadata, making it a valuable resource for training and evaluating automatic speech recognition (ASR) systems. TED-LIUM captures a wide variety of speaking styles, accents, and topics such as technology, science, education, and culture. Multiple versions of the dataset (v1, v2, v3) have been released, with v3 offering over 450 hours of speech. The dataset is licensed under Creative Commons BY-NC-ND 3.0, which allows non-commercial use but prohibits derivative works.\n",
    "\n",
    "The dataset can be fount at: https://huggingface.co/datasets/LIUM/tedlium/tree/main/TEDLIUM_release3/speaker-adaptation\n",
    "\n",
    "### **Features & Characteristics**\n",
    "- **Sourced from**<br>\n",
    "    Audio recordings of TED conferences.\n",
    "- **Typical Topics**<br>\n",
    "    Wide variety of specialized topics.\n",
    "- **Accents & Languages**<br>\n",
    "    - Only English.\n",
    "    - Supports different English accents since speakers are from all over the world.\n",
    "        - There is no official labeling of accent type in the metadata\n",
    "- **Noise**<br>\n",
    "    - people clapping\n",
    "    - music <br>\n",
    "        Actually, in the test subset, there is a clip \"AimeeMullins_2009P\" where at the begging it has a music. I tried to input the segment of the music into the model Whisper and got the following output: \"Yes\". There was no one saying anything at that segment\n",
    "    - Audience reactions\n",
    "        - Laughter\n",
    "        - applause\n",
    "        - cheering\n",
    "        - murmuring during or after a speaker‚Äôs statement\n",
    "    - Microphone noises: Occasional clipping, pops, or breath sounds due to microphone proximity or handling\n",
    "    - Ambient room noise: Subtle echo or reverberation from large auditoriums or conference rooms\n",
    "    - Slide click sounds\n",
    "    - Speech overlaps: In rare Q&A segments or dialogue-style talks, slight overlaps may occur.\n",
    "\n",
    "### **Some Issues To Consider**\n",
    "- **Typos**<br>\n",
    "    In the test subset, specificaly in AimeeMullins_2009P, the first transcription was:<br>\n",
    "    \"i 'd like to share with you a discovery that i made a few months ago while writing an article for italian wired i always keep my thesaurus handy whenever i 'm writing anything but\"<br>\n",
    "    Meanwhile the whisper model predicted the transcription as:<br>\n",
    "    \"I'd like to share with you a discovery that I made a few months ago while writing an article for Italian Wired. I always keep my bizarre as handy whenever I'm writing anything but...\"<br>\n",
    "    In the original transcription it is \"i 'd\", while the model predicted it to be \"I'd\". This might be considered as a prediction error, while in fact, it is correct\n",
    "- **Incomplete sentences**: as it clear from the previous example, the sentence got cut after the word \"but\", this led the model to add \"...\" at the end. This issue can be solved by normalizing\n",
    "\n",
    "### **Missing Features**\n",
    "The dataset doesn't contain irregular language such as mathematical expressions.\n",
    "\n",
    "### **Dataset Structure & Details**<br>\n",
    "Each sample contains the following values:\n",
    "- **audio:** This is a dictionary contains three values:\n",
    "    - **file:** The absolute path file in .flac format\n",
    "    - **array:** An array of floats representing the loadede corresponding audio file\n",
    "    - **sampling_rate:** The sampling rate of the corresponding audio file\n",
    "- **file:** A path to the downloaded audio file in .sph format\n",
    "- **text:** Transcription\n",
    "- **gender:** The gender of the speaker\n",
    "- **id:** A unique ID for the sample\n",
    "- **speaker_id:** A unique ID for the speaker\n",
    "\n",
    "The data samples are split as follows:\n",
    "| Split | Release 1 | Release 2 | Release 3 |\n",
    "|----------|:----------:|:----------:|:----------:|\n",
    "| Train | 56,803 | 92,973 | 268,263 |\n",
    "| Validation | 591 | 591 | 591 |\n",
    "| Test | 1,469 | 1,469 | 1,469 |\n",
    "\n",
    "### **References**\n",
    "https://www.openslr.org/51/<br>\n",
    "https://www.innovatiana.com/en/datasets/ted-lium-dataset<br>\n",
    "https://huggingface.co/datasets/LIUM/tedlium<br>\n",
    "\n",
    "### **Exploring The Dataset**\n",
    "Unfortunately, to download a small subset of the dataset you'll need to download the whole 164.029 GBytes. And they made it illegal to repackage it or modify it in any way, due to the [CC BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/4.0/deed.en). Thus, I won't explore it like I always do. But, I <ins>**might**</ins> just create my own subset and modify it and use it privately for testing later. But I <ins>**might not**</ins> do this since it is illegal üòâ\n",
    "\n",
    "Here is an example code (I got it from https://huggingface.co/datasets/LIUM/tedlium#example) showing how to officialy and legally download the dataset:\n",
    "```Python\n",
    "from datasets import load_dataset\n",
    "\n",
    "tedlium = load_dataset(\"LIUM/tedlium\", \"release1\") # for Release 1\n",
    "\n",
    "# see structure\n",
    "print(tedlium)\n",
    "\n",
    "# load audio sample on the fly\n",
    "audio_input = tedlium[\"train\"][0][\"audio\"]  # first decoded audio sample\n",
    "transcription = tedlium[\"train\"][0][\"text\"]  # first transcription\n",
    "```\n",
    "\n",
    "### **4. MathSpeech** <a id=\"datasets-mathspeech\"></a>\n",
    "\n",
    "This dataset contains 1101 audio clips sourced from [MIT OpenCourseWare lectures](https://ocw.mit.edu/about/), where mathematical expressions are spoken aloud in natural English (e.g., ‚Äúe to the power of i x equals cosine of x plus i sine of x‚Äù). Each audio file is paired with two transcripts, one that represents the mathematical content in plain English words, making it directly compatible with standard ASR models, and the other represents the mathematical content using LaTex. This makes MathSpeech useful for tasks like spoken equation recognition, speech-to-LaTeX conversion, and evaluating ASR systems on non-standard, symbol-heavy language expressed verbally. The dataset includes diverse speakers and real lecture-style noise, offering a realistic testbed for robust math-aware speech models.\n",
    "\n",
    "The dataste can be found here: https://huggingface.co/datasets/AAAI2025/MathSpeech\n",
    "\n",
    "### **Features & Characteristics**\n",
    "- **Sourced from**<br>\n",
    "    [MIT OpenCourseWare lectures](https://ocw.mit.edu/about/).\n",
    "- **Accents & Languages**<br>\n",
    "    - U.S. English.\n",
    "- **Noise**<br>\n",
    "    - Room reverberation\n",
    "    - Breath sounds\n",
    "    - Microphone artifacts\n",
    "    - Background hum: Low-frequency static or electronic hum from recording equipment\n",
    "    - Page flipping\n",
    "    - chalkboard use\n",
    "    - Speech disfluencies\n",
    "- **Transcripts**<br>\n",
    "    Each sample comes with two types of transcripts:\n",
    "    - Plain English: \"ax plus by plus cz equals d\"\n",
    "    - LaTex: \"$ax + by + cz = d$\"\n",
    "\n",
    "### **Some Issues To Consider**\n",
    "- Inconsistent Letter Cases: Most of the samples plain English transcripts contain only small letters, but few samples contain capital letters.\n",
    "- Inconsistent Punctuations: Most of the samples plain English transcripts don't contain punctuations, but there are very few samples that do.\n",
    "\n",
    "### **Dataset Structure & Details**<br>\n",
    "Each sample contains the following values:\n",
    "- **audio:** This is a dictionary contains three values:\n",
    "    - **file:** A path to the audio file\n",
    "    - **array:** An array of floats representing the loadede corresponding audio file\n",
    "    - **sampling_rate:** The sampling rate of the corresponding audio file. By default, it is 48000Hz, you might need to down sample it to 16000Hz to be compatible with some models\n",
    "- **text:** Transcription in plain English\n",
    "- **LaTex:** Transcription in LaTex\n",
    "- **Source:** A youtube link to the sourced lecture\n",
    "\n",
    "There is only one split, which is the training split.\n",
    "\n",
    "### **References**\n",
    "https://huggingface.co/datasets/AAAI2025/MathSpeech<br>\n",
    "https://arxiv.org/html/2412.15655v1<br>\n",
    "https://github.com/hyeonsieun/mathspeech?tab=readme-ov-file<br>\n",
    "\n",
    "### **Exploring The Dataset**\n",
    "- #### **Download It**\n",
    "```Python\n",
    "from datasets import load_dataset, Audio\n",
    "\n",
    "dataset = load_dataset(\"AAAI2025/MathSpeech\", split=\"train\")\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "dataset\n",
    "```\n",
    "**output:**\n",
    "```Python\n",
    "Dataset({\n",
    "    features: ['audio', 'transcription', 'LaTeX', 'Source'],\n",
    "    num_rows: 1101\n",
    "})\n",
    "```\n",
    "\n",
    "- #### **Dataset Format**\n",
    "```Python\n",
    "dataset[0]\n",
    "```\n",
    "**output:**\n",
    "```Python\n",
    "{'audio': {'path': '1.mp3',\n",
    "  'array': array([-2.32830644e-10, -2.32830644e-10, -1.39698386e-09, ...,\n",
    "          5.56293176e-03,  4.59455093e-03,  2.39031739e-03]),\n",
    "  'sampling_rate': 16000},\n",
    " 'transcription': 'ax plus by plus cz equals d',\n",
    " 'LaTeX': '$ax + by + cz = d$',\n",
    " 'Source': 'https://www.youtube.com/watch?v=YBajUR3EFSM'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ba734",
   "metadata": {},
   "source": [
    "## **4Ô∏è‚É£ Preprocessing** <a id=\"preprocessing\"></a>\n",
    "\n",
    "### There are some common issues emerged while I was testing the models and exploring the datasets. These issues weren't produced because the models or datasets them selfs, but because of the nature of human language, specifically, the English language (Since I didn't use any other language at all).\n",
    "\n",
    "### In this section, I will define these issues, and prove whether if they could affect the evaluation process or not. The reason why it is important to know if they affect the rvaluation is because these issues might produce misleading results, pushing us into thinking that the results represents the models and datasets performance, while the real problem might be in the human language nature.\n",
    "\n",
    "### All the transcriptions I'll use in the next examples will get normalized first (i.e. All letter cases will get converted into one case, and all punctuations will get removed), and all the predicted words must be 100% accurate, by accurate I mean represent the correct word, e.g. the words '2' and 'two' are the same. These preperations ensure that any error won't be caused by the model or the dataset, but instead, the human language nature itself.\n",
    "\n",
    "### To normalize the transcriptions, I used the following code:\n",
    "```Python\n",
    "from jiwer import Compose, ToLowerCase, RemovePunctuation, RemoveMultipleSpaces, Strip\n",
    "\n",
    "transform = Compose([\n",
    "    ToLowerCase(),\n",
    "    RemovePunctuation(), # this can produce wrong results in some cases, I will discuss this later\n",
    "    RemoveMultipleSpaces(),\n",
    "    Strip()\n",
    "])\n",
    "```\n",
    "\n",
    "### Example:\n",
    "```Python\n",
    "transform(\"In these cases, skin tone and hair color are not so important.\")\n",
    "```\n",
    "**output:**\n",
    "```Python\n",
    "'in these cases skin tone and hair color are not so important'\n",
    "```\n",
    "\n",
    "### **The Issues**\n",
    "- **Spelling for different accents**: The same word might have different spelling for different accents, and all are correct. For example, the word color/colour:<br>\n",
    "    - In the Common Vocie dataset:\n",
    "        - Spelled color in:\n",
    "            - common_voice_en_27638325.mp3 : \"Its shape and coloration is reminiscent of a brown trout.\"\n",
    "            - common_voice_en_19757703.mp3 : \"Coalitions in the Bundestag and state legislators are often described by party colors.\"\n",
    "            - common_voice_en_25467856.mp3 : \"In these cases, skin tone and hair color are not so important.\"\n",
    "            - common_voice_en_31206949.mp3 : \"The color is mostly red with yellow highlights near the crown.\"\n",
    "            - common_voice_en_28527878.mp3 : \"Legal specification for the shades of the national colors has also changed with time.\"\n",
    "            - common_voice_en_32638824.mp3 : \"Studies have found that students of color are disproportionately affected.\"\n",
    "            - common_voice_en_31159638.mp3 : \"The film was shot in color with mono sound.\"\n",
    "        - Spelled colour in:\n",
    "            - common_voice_en_18276812.mp3 : \"What a strange mauve colour!\"\n",
    "            - common_voice_en_17272411.mp3 : \"Somehow, the purple colour faded to gray.\"\n",
    "\n",
    "    The same thing occurs in the other datasets.\n",
    "\n",
    "- **Homophone involving a symbol or numeral**: For examples, the words \"two\", and \"2\" pronounced the same. The ASR model might hear \"2\" and output \"two\", and vice versa.<br>\n",
    "I used the ASR model \"Whisper\" to test this, and it really happened:\n",
    "    - In MathSpeech the sample 2.mp3:\n",
    "        - Prediction: \"x plus 5y plus 10z equals 0.\"\n",
    "        - Reference : \"x plus 5y plus 10z equals zero\"<br>\n",
    "\n",
    "    While \"0\" and \"zero\" are the same, they are written differently.<br>\n",
    "    The same thing occurs in the other datasets.\n",
    "\n",
    "### **Will These Issues Affects The Evaluation Results?**\n",
    "The following code tests the previously mentioned issues using the WER metric (Which is mentioned in the next section):\n",
    "```Python\n",
    "from jiwer import wer\n",
    "\n",
    "# test for homophone involving a symbol or numeral\n",
    "Ex1_A = 'Saturday, August 15th. The sea unbroken all round. No land in sight.'\n",
    "Ex1_B = 'SATURDAY AUGUST FIFTEENTH THE SEA UNBROKEN ALL ROUND NO LAND IN SIGHT'\n",
    "Ex1_C = 'Saturday, August fifteenth. The sea unbroken all round. No land in sight.'\n",
    "\n",
    "print('test for homophone involving a symbol or numeral')\n",
    "print(f'Whith the issue: {wer(transform(Ex1_A), transform(Ex1_B))}')\n",
    "print(f'Whithout the issue: {wer(transform(Ex1_B), transform(Ex1_C))}\\n')\n",
    "\n",
    "# test for different spelling\n",
    "Ex2_A = 'Words was it their colors'\n",
    "Ex2_B = 'words, was. It; their colours'\n",
    "Ex2_C = 'Words was it their colours'\n",
    "\n",
    "print('test for different spelling')\n",
    "print(f'Whith the issue: {wer(transform(Ex2_A), transform(Ex2_B))}')\n",
    "print(f'Whithout the issue: {wer(transform(Ex2_B), transform(Ex2_C))}')\n",
    "```\n",
    "**output:**\n",
    "```Python\n",
    "test for homophone involving a symbol or numeral\n",
    "Whith the issue: 0.08333333333333333\n",
    "Whithout the issue: 0.0\n",
    "\n",
    "test for different spelling\n",
    "Whith the issue: 0.2\n",
    "Whithout the issue: 0.0\n",
    "```\n",
    "### Sure enough, this will affect the evaluation results.\n",
    "\n",
    "### **Preprocessing**\n",
    "Here are some steps that I hope could minimize the problem:\n",
    "\n",
    "### **1. Unify Spellings**\n",
    "#### To solve the spelling problem, I will use a function that unify the spellings:\n",
    "```Python\n",
    "from breame.spelling import get_american_spelling # or get_british_spelling\n",
    "\n",
    "get_american_spelling('DISCOLOURED')\n",
    "```\n",
    "**output:**\n",
    "```Python\n",
    "'discolored'\n",
    "```\n",
    "\n",
    "#### Notice how the function didn't just solve the spelling problem, it also changed all the letters cases into small. Thus, there is no need to use \"ToLowerCase()\" no more.\n",
    "\n",
    "#### But this function is not totaly ready yet, it only works with one word, if you tried to use it with a full sentence, then the spelling doesn't change.\n",
    "\n",
    "### **2. Combine With Compose**\n",
    "```Python\n",
    "from breame.spelling import get_american_spelling # or get_british_spelling\n",
    "from jiwer import Compose, RemovePunctuation, RemoveMultipleSpaces, Strip\n",
    "\n",
    "def american_spelling(text):\n",
    "    result = ''\n",
    "\n",
    "    for c in text.split():\n",
    "        result += get_american_spelling(c) + ' '\n",
    "    \n",
    "    return result\n",
    "\n",
    "transform = Compose([\n",
    "    american_spelling,\n",
    "    RemovePunctuation(),\n",
    "    RemoveMultipleSpaces(),\n",
    "    Strip()\n",
    "])\n",
    "\n",
    "text = 'IN, THE. it\\'s LIGHT OF THE MOON I SAW A KNIFE RED WITH BLOOD AND MY HAND TOO WAS ALSO DISCOLOURED'\n",
    "\n",
    "transform(text)\n",
    "```\n",
    "**output:**\n",
    "```Python\n",
    "'in the its light of the moon i saw a knife red with blood and my hand too was also discolored'\n",
    "```\n",
    "\n",
    "#### Notice how the third word changed from \"it's\" into \"its\", which has a different meaning. This happened because the ' was deleted by the RemovePunctuation(). This can be solved as follows:\n",
    "\n",
    "### **3. ExpandCommonEnglishContractions()**\n",
    "```Python\n",
    "from breame.spelling import get_american_spelling # or get_british_spelling\n",
    "from jiwer import Compose, RemovePunctuation, RemoveMultipleSpaces, Strip, ExpandCommonEnglishContractions\n",
    "\n",
    "def american_spelling(text):\n",
    "    result = ''\n",
    "\n",
    "    for c in text.split():\n",
    "        result += get_american_spelling(c) + ' '\n",
    "    \n",
    "    return result\n",
    "\n",
    "transform = Compose([\n",
    "    american_spelling,\n",
    "    ExpandCommonEnglishContractions(),\n",
    "    RemovePunctuation(),\n",
    "    RemoveMultipleSpaces(),\n",
    "    Strip()\n",
    "])\n",
    "```\n",
    "**output:**\n",
    "```Python\n",
    "'in the it is light of the moon i saw a knife red with blood and my hand too was also discolored'\n",
    "```\n",
    "\n",
    "#### Now \"it's\" changed into \"it\" and \"is\", solving the issue.\n",
    "\n",
    "#### I will stop here and won't face the second issue disccused previously because the time is running.\n",
    "\n",
    "#### Also my preprocessings might cause other new issues! I'll see whether if the errors are less or not this way, if it is less, Then I won! The examples I showed were picked carefuly to prove my points, but I am not sure how things gonna be when testing full datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f240a",
   "metadata": {},
   "source": [
    "## **5Ô∏è‚É£ Evaluation Metrics** <a id=\"evaluation-metrics\"></a>\n",
    "\n",
    "### I am using only one metric since it does the job perfectly and I got no more time to spend on such details. The metric Is called Word Error Rate (WER), which is a common metric used to evaluate the performance of automatic speech recognition (ASR) systems. It measures how accurately a system transcribes spoken language into text by comparing the system's output to a correct version (reference). WER is particularly useful because it provides a straightforward, interpretable score: the lower the WER, the better the transcription quality. This metric accounts for various types of errors, including incorrect words, missing words, and extra words.\n",
    "\n",
    "### The formula for WER is:\n",
    "\n",
    "### $$\\text{WER} = \\frac{S + D + I}{N}$$\n",
    "\n",
    "### Where:\n",
    "#### - S = the number of substitutions (wrong words),\n",
    "#### - D = the number of deletions (missing words),\n",
    "#### - I = the number of insertions (extra words),\n",
    "#### - N = the total number of words in the reference.\n",
    "\n",
    "### This equation essentially calculates the minimum number of operations required to transform the hypothesis into the reference, normalized by the total number of words in the reference. WER can exceed 1.0 (or 100%) if the number of errors is greater than the number of reference words.\n",
    "\n",
    "### I won't implement this my self, instead, I'll use a very simply and useful function as follows:\n",
    "```Python\n",
    "from jiwer import wer\n",
    "\n",
    "wer('This is a test for...', 'that was A for...')\n",
    "```\n",
    "**output:**\n",
    "```Python\n",
    "0.8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd08f9a",
   "metadata": {},
   "source": [
    "## **6Ô∏è‚É£ Preparing The Testing Codes** <a id=\"preparing-the-testing-codes\"></a>\n",
    "\n",
    "### In this section, I will lay down all the codes needed to conduct the experiment.\n",
    "\n",
    "### Note, these codes were originaly written in a notebook, this is why tou might see some repeated lines since because sometimes I execute cells seperately and out of order, so I might need to repeat things.\n",
    "\n",
    "### **1. Login To Hugging Face** <a id=\"preparing-the-testing-codes-login-to-hugging-face\"></a>\n",
    "\n",
    "#### Most datasets and models will be downloaded from Hugging Face, thus, Hugging Face requires you to login in first in order to being able to download any thing. This is done by the following code:\n",
    "\n",
    "```Python\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"You access token's value\")\n",
    "```\n",
    "#### I didn't mention how to get your access token value, just google it.\n",
    "\n",
    "### **2. Create Classes For The ASR Models** <a id=\"preparing-the-testing-codes-create-classes-for-the-asr-models\"></a>\n",
    "\n",
    "#### I created classes for each asr model despite its size/version to simplify thongs out later:\n",
    "\n",
    "#### 1. Whisper\n",
    "\n",
    "```Python\n",
    "%pip install openai-whisper\n",
    "\n",
    "import whisper\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# sizes:\n",
    "# tiny\n",
    "# base\n",
    "# small\n",
    "# medium\n",
    "# turbo\n",
    "# large\n",
    "\n",
    "class Whisper:\n",
    "    def __init__(self, size):\n",
    "        self.model = whisper.load_model(size)\n",
    "\n",
    "    def transcribe(self, audio_array, sample_rate):\n",
    "        return self.model.transcribe(audio_array, language=\"en\", fp16=False)[\"text\"]\n",
    "\n",
    "    def __del__(self):\n",
    "        del self.model\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache\n",
    "```\n",
    "\n",
    "#### 2. Wav2Vec2\n",
    "\n",
    "```Python\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# versions:\n",
    "# wav2vec2-base\n",
    "# wav2vec2-base-960h\n",
    "# wav2vec2-large\n",
    "# wav2vec2-large-960h\n",
    "# wav2vec2-large-960h-lv60-self\n",
    "# wav2vec2-large-xlsr-53\n",
    "\n",
    "class Wav2Vec2:\n",
    "    def __init__(self, version):\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(\"facebook/\" + version)\n",
    "        self.model = Wav2Vec2ForCTC.from_pretrained(\"facebook/\" + version)\n",
    "        self.model.eval()\n",
    "\n",
    "    def transcribe(self, audio_array, sample_rate):\n",
    "        input_values = self.processor(audio_array, return_tensors=\"pt\", sampling_rate=sample_rate).input_values\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_values).logits\n",
    "        \n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        return self.processor.decode(predicted_ids[0])\n",
    "\n",
    "    def __del__(self):\n",
    "        del self.model\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache\n",
    "```\n",
    "\n",
    "#### 3. Parakeet\n",
    "\n",
    "```Python\n",
    "%pip install -U nemo_toolkit[\"asr\"]\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# versions:\n",
    "# parakeet-tdt-0.6b-v2\n",
    "# parakeet-tdt-1.1b\n",
    "# parakeet-rnnt-1.1b\n",
    "# parakeet-ctc-0.6b\n",
    "# parakeet-ctc-1.1b\n",
    "\n",
    "class Parakeet:\n",
    "    def __init__(self, version):\n",
    "        self.model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/\" + version)\n",
    "\n",
    "    def transcribe(self, audio_array, sample_rate):\n",
    "        return self.model.transcribe(audio_array, verbose=False)[0].text\n",
    "\n",
    "    def __del__(self):\n",
    "        del self.model\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "```\n",
    "\n",
    "### **3. Dataset Class** <a id=\"preparing-the-testing-codes-datasets-class\"></a>\n",
    "\n",
    "#### Since datasets have different formats and features names, I created the following class to unify dealing with them:\n",
    "\n",
    "#### Note the \"[Important Notes](#preparing-the-testing-codes-important-notes)\" subsection where I ralked about the \"BinAlsadiq/tedlium_3_test_segments\" dataset which doesn't exist üòâ\n",
    "\n",
    "```Python\n",
    "from datasets import load_dataset, Audio\n",
    "\n",
    "class DatasetAPI:\n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        \n",
    "        if dataset_name == 'tedlium_3_test_segments':\n",
    "            self.dataset = load_dataset(\"BinAlsadiq/tedlium_3_test_segments\", split='test', trust_remote_code=True)\n",
    "        elif dataset_name == 'librispeech_test_clean':\n",
    "            self.dataset = load_dataset(\"BinAlsadiq/librispeech_test_clean\", split='test', trust_remote_code=True)\n",
    "        elif dataset_name == 'librispeech_test_other':\n",
    "            self.dataset = load_dataset(\"BinAlsadiq/librispeech_test_other\", split='test', trust_remote_code=True)\n",
    "        elif dataset_name == 'common_voice_13_0_en_test':\n",
    "            self.dataset = load_dataset(\"BinAlsadiq/common_voice_13_0_en_test\", split='test', trust_remote_code=True)\n",
    "        elif dataset_name == 'MathSpeech':\n",
    "            self.dataset = load_dataset(\"AAAI2025/MathSpeech\", split=\"train\", trust_remote_code=True)\n",
    "            self.dataset = self.dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._index < len(self.dataset):\n",
    "            sample = self.dataset[self._index]\n",
    "            self._index += 1\n",
    "            transcription_key = 'transcription' if self.dataset_name == 'MathSpeech' else 'sentence' if self.dataset_name == 'common_voice_13_0_en_test' else 'text'\n",
    "            \n",
    "            return {\n",
    "                'audio_array' : sample[\"audio\"][\"array\"].astype(\"float32\"),\n",
    "                'sampling_rate' : sample[\"audio\"][\"sampling_rate\"],\n",
    "                'transcription' : sample[transcription_key]\n",
    "            }\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "```\n",
    "\n",
    "### **4. Preprocessors** <a id=\"preparing-the-testing-codes-preprocessors\"></a>\n",
    "\n",
    "#### I created two preprocessors as shown in the code below, remember their names because they will be used later on.\n",
    "\n",
    "```Python\n",
    "%pip install breame\n",
    "\n",
    "from breame.spelling import get_american_spelling # or get_british_spelling\n",
    "from jiwer import Compose, RemovePunctuation, RemoveMultipleSpaces, Strip, ExpandCommonEnglishContractions, ToLowerCase\n",
    "\n",
    "def american_spelling(text):\n",
    "    result = ''\n",
    "\n",
    "    for c in text.split():\n",
    "        result += get_american_spelling(c) + ' '\n",
    "    \n",
    "    return result\n",
    "\n",
    "preprocessorA = Compose([\n",
    "    ToLowerCase(),\n",
    "    RemovePunctuation(),\n",
    "    RemoveMultipleSpaces(),\n",
    "    Strip()\n",
    "])\n",
    "\n",
    "preprocessorB = Compose([\n",
    "    american_spelling,\n",
    "    ExpandCommonEnglishContractions(),\n",
    "    RemovePunctuation(),\n",
    "    RemoveMultipleSpaces(),\n",
    "    Strip()\n",
    "])\n",
    "```\n",
    "\n",
    "### **5. Testing Loop** <a id=\"preparing-the-testing-codes-testing-loop\"></a>\n",
    "\n",
    "#### The following code is a testing loop that takes two inputs, the first one is a model to test, the second is a list of datasets to test the model on. The loop produce three evaluation results for each dataset. It is shown clearly in the code what these results are, so I won't explain them.\n",
    "\n",
    "```Python\n",
    "from jiwer import wer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test(model, datasets):\n",
    "    wer_list = []\n",
    "    Awer_list = []\n",
    "    Bwer_list = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        wer_sum = 0\n",
    "        Awer_sum = 0\n",
    "        Bwer_sum = 0\n",
    "        \n",
    "        for sample in tqdm(dataset):\n",
    "            prediction = model.transcribe(sample[\"audio_array\"], sample[\"sampling_rate\"])\n",
    "            reference = sample[\"transcription\"]\n",
    "        \n",
    "            wer_sum += wer(reference, prediction)\n",
    "            Awer_sum += wer(preprocessorA(reference), preprocessorA(prediction))\n",
    "            Bwer_sum += wer(preprocessorB(reference), preprocessorB(prediction))\n",
    "\n",
    "        wer_list.append({ dataset.dataset_name : wer_sum / len(dataset)})\n",
    "        Awer_list.append({ dataset.dataset_name : Awer_sum / len(dataset)})\n",
    "        Bwer_list.append({ dataset.dataset_name : Bwer_sum / len(dataset)})\n",
    "\n",
    "    return {\n",
    "        'WER without preprocessing' : wer_list, \n",
    "        'WER with type A preprocessing' : Awer_list, \n",
    "        'WER with type B preprocessing' : Bwer_list\n",
    "    }\n",
    "```\n",
    "\n",
    "### **6. Start The Tests** <a id=\"preparing-the-testing-codes-start-the-tests\"></a>\n",
    "\n",
    "#### Now all what you need to do is to test the model you want.\n",
    "\n",
    "#### The following code will do so, and will display and store the results in a \".csv\" file:\n",
    "\n",
    "```Python\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "\n",
    "datasets = [\n",
    "    DatasetAPI('tedlium_3_test_segments'),\n",
    "    DatasetAPI('librispeech_test_clean'),\n",
    "    DatasetAPI('librispeech_test_other'),\n",
    "    DatasetAPI('common_voice_13_0_en_test'),\n",
    "    DatasetAPI('MathSpeech')\n",
    "]\n",
    "\n",
    "model = Whisper('tiny')\n",
    "\n",
    "results = DataFrame(test(model, datasets))\n",
    "results.to_csv('whisper-tiny.csv')\n",
    "print(results)\n",
    "\n",
    "del model\n",
    "```\n",
    "\n",
    "### **7. Important Notes** <a id=\"preparing-the-testing-codes-important-notes\"></a>\n",
    "\n",
    "- #### **The wav2vec2-base model is pretrained on features extracting only and it is not read to be used as an ASR. This will cause the performance to be very poor as will be shown in the next section. I should fine tune it first, but now I am only testing the models as they are, later I will decide if fine-tuning is necessary.**:\n",
    "\n",
    "- #### **You can't download wav2vec2-large and wav2vec2-large-xlsr-53 like the other versions. There is a reason for that, but I don't have time, and it doesn't really matter. Thus, these two models will be ignored.**\n",
    "\n",
    "- #### **The \"BinAlsadiq/tedlium_3_test_segments\" dataset doesn't really exist, thus, all the related results are fake üòâ. I can't make my own subset out of the TedLIUM dataset since it is illegal.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba60ad",
   "metadata": {},
   "source": [
    "## **7Ô∏è‚É£ Results** <a id=\"results\"></a>\n",
    "\n",
    "### **whisper-tiny**:\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>WER without preprocessing</th>\n",
    "      <th>WER with type A preprocessing</th>\n",
    "      <th>WER with type B preprocessing</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>'tedlium_3_test_segments'</th>\n",
    "      <td>0.425206</td>\n",
    "      <td>0.254331</td>\n",
    "      <td>0.223201</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_clean'</th>\n",
    "      <td>0.993131</td>\n",
    "      <td>0.100179</td>\n",
    "      <td>0.098831</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_other'</th>\n",
    "      <td>0.995785</td>\n",
    "      <td>0.190091</td>\n",
    "      <td>0.184871</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'common_voice_13_0_en_test'</th>\n",
    "      <td>0.472529</td>\n",
    "      <td>0.423461</td>\n",
    "      <td>0.424334</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'MathSpeech'</th>\n",
    "      <td>0.333016</td>\n",
    "      <td>0.203956</td>\n",
    "      <td>0.204865</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "### **whisper-base**:\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>WER without preprocessing</th>\n",
    "      <th>WER with type A preprocessing</th>\n",
    "      <th>WER with type B preprocessing</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>'tedlium_3_test_segments'</th>\n",
    "      <td>0.428846</td>\n",
    "      <td>0.256111</td>\n",
    "      <td>0.213590</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_clean'</th>\n",
    "      <td>0.982150</td>\n",
    "      <td>0.086610</td>\n",
    "      <td>0.082629</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_other'</th>\n",
    "      <td>0.991771</td>\n",
    "      <td>0.160075</td>\n",
    "      <td>0.155004</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'common_voice_13_0_en_test'</th>\n",
    "      <td>0.367940</td>\n",
    "      <td>0.309804</td>\n",
    "      <td>0.311929</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'MathSpeech'</th>\n",
    "      <td>0.275864</td>\n",
    "      <td>0.148422</td>\n",
    "      <td>0.149331</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "### **whisper-small**:\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>WER without preprocessing</th>\n",
    "      <th>WER with type A preprocessing</th>\n",
    "      <th>WER with type B preprocessing</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>'tedlium_3_test_segments'</th>\n",
    "      <td>0.529205</td>\n",
    "      <td>0.357324</td>\n",
    "      <td>0.314501</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_clean'</th>\n",
    "      <td>0.978684</td>\n",
    "      <td>0.053592</td>\n",
    "      <td>0.050267</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_other'</th>\n",
    "      <td>0.986757</td>\n",
    "      <td>0.098921</td>\n",
    "      <td>0.092799</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'common_voice_13_0_en_test'</th>\n",
    "      <td>0.304672</td>\n",
    "      <td>0.246675</td>\n",
    "      <td>0.240676</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'MathSpeech'</th>\n",
    "      <td>0.264789</td>\n",
    "      <td>0.137663</td>\n",
    "      <td>0.137663</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "### **whisper-medium**:\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>WER without preprocessing</th>\n",
    "      <th>WER with type A preprocessing</th>\n",
    "      <th>WER with type B preprocessing</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>'tedlium_3_test_segments'</th>\n",
    "      <td>0.492625</td>\n",
    "      <td>0.312032</td>\n",
    "      <td>0.268383</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_clean'</th>\n",
    "      <td>0.980107</td>\n",
    "      <td>0.051063</td>\n",
    "      <td>0.050851</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_other'</th>\n",
    "      <td>0.981331</td>\n",
    "      <td>0.071449</td>\n",
    "      <td>0.066030</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'common_voice_13_0_en_test'</th>\n",
    "      <td>0.236713</td>\n",
    "      <td>0.172767</td>\n",
    "      <td>0.164299</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'MathSpeech'</th>\n",
    "      <td>0.245444</td>\n",
    "      <td>0.105051</td>\n",
    "      <td>0.109210</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "### **whisper-turbo**:\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>WER without preprocessing</th>\n",
    "      <th>WER with type A preprocessing</th>\n",
    "      <th>WER with type B preprocessing</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>'tedlium_3_test_segments'</th>\n",
    "      <td>0.561731</td>\n",
    "      <td>0.378410</td>\n",
    "      <td>0.335652</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_clean'</th>\n",
    "      <td>0.981520</td>\n",
    "      <td>0.042134</td>\n",
    "      <td>0.041912</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_other'</th>\n",
    "      <td>0.985048</td>\n",
    "      <td>0.056626</td>\n",
    "      <td>0.051830</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'common_voice_13_0_en_test'</th>\n",
    "      <td>0.247533</td>\n",
    "      <td>0.191593</td>\n",
    "      <td>0.187984</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'MathSpeech'</th>\n",
    "      <td>0.293834</td>\n",
    "      <td>0.159705</td>\n",
    "      <td>0.162614</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "### **whisper-large**:\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>WER without preprocessing</th>\n",
    "      <th>WER with type A preprocessing</th>\n",
    "      <th>WER with type B preprocessing</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>'tedlium_3_test_segments'</th>\n",
    "      <td>0.520963</td>\n",
    "      <td>0.344325</td>\n",
    "      <td>0.301300</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_clean'</th>\n",
    "      <td>0.977587</td>\n",
    "      <td>0.038080</td>\n",
    "      <td>0.037809</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_other'</th>\n",
    "      <td>0.987934</td>\n",
    "      <td>0.058438</td>\n",
    "      <td>0.051702</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'common_voice_13_0_en_test'</th>\n",
    "      <td>0.242730</td>\n",
    "      <td>0.183063</td>\n",
    "      <td>0.176834</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'MathSpeech'</th>\n",
    "      <td>0.260550</td>\n",
    "      <td>0.105496</td>\n",
    "      <td>0.106405</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "### **wav2vec2-base**:\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>WER without preprocessing</th>\n",
    "      <th>WER with type A preprocessing</th>\n",
    "      <th>WER with type B preprocessing</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>'tedlium_3_test_segments'</th>\n",
    "      <td>1.276548</td>\n",
    "      <td>1.276148</td>\n",
    "      <td>1.277075</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_clean'</th>\n",
    "      <td>0.999677</td>\n",
    "      <td>0.999677</td>\n",
    "      <td>0.999687</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_other'</th>\n",
    "      <td>0.999778</td>\n",
    "      <td>0.999778</td>\n",
    "      <td>0.999778</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'common_voice_13_0_en_test'</th>\n",
    "      <td>1.211980</td>\n",
    "      <td>1.211980</td>\n",
    "      <td>1.218914</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'MathSpeech'</th>\n",
    "      <td>1.003636</td>\n",
    "      <td>1.003110</td>\n",
    "      <td>1.004019</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "### **wav2vec2-base-960h**:\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>WER without preprocessing</th>\n",
    "      <th>WER with type A preprocessing</th>\n",
    "      <th>WER with type B preprocessing</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>'tedlium_3_test_segments'</th>\n",
    "      <td>1.005146</td>\n",
    "      <td>0.249087</td>\n",
    "      <td>0.210693</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_clean'</th>\n",
    "      <td>0.031998</td>\n",
    "      <td>0.031998</td>\n",
    "      <td>0.029351</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_other'</th>\n",
    "      <td>0.088620</td>\n",
    "      <td>0.088494</td>\n",
    "      <td>0.082035</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'common_voice_13_0_en_test'</th>\n",
    "      <td>1.066622</td>\n",
    "      <td>0.492667</td>\n",
    "      <td>0.487650</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'MathSpeech'</th>\n",
    "      <td>1.046253</td>\n",
    "      <td>0.626175</td>\n",
    "      <td>0.628175</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "### **wav2vec2-large-960h**:\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>WER without preprocessing</th>\n",
    "      <th>WER with type A preprocessing</th>\n",
    "      <th>WER with type B preprocessing</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>'tedlium_3_test_segments'</th>\n",
    "      <td>1.027285</td>\n",
    "      <td>0.261928</td>\n",
    "      <td>0.222394</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_clean'</th>\n",
    "      <td>0.032082</td>\n",
    "      <td>0.032082</td>\n",
    "      <td>0.030782</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_other'</th>\n",
    "      <td>0.056778</td>\n",
    "      <td>0.055647</td>\n",
    "      <td>0.050764</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'common_voice_13_0_en_test'</th>\n",
    "      <td>1.064227</td>\n",
    "      <td>0.422171</td>\n",
    "      <td>0.416975</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'MathSpeech'</th>\n",
    "      <td>1.042963</td>\n",
    "      <td>0.588449</td>\n",
    "      <td>0.598544</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "### **wav2vec2-large-960h-lv60-self**:\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>WER without preprocessing</th>\n",
    "      <th>WER with type A preprocessing</th>\n",
    "      <th>WER with type B preprocessing</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>'tedlium_3_test_segments'</th>\n",
    "      <td>1.005705</td>\n",
    "      <td>0.225617</td>\n",
    "      <td>0.186261</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_clean'</th>\n",
    "      <td>0.023617</td>\n",
    "      <td>0.023617</td>\n",
    "      <td>0.022159</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_other'</th>\n",
    "      <td>0.047817</td>\n",
    "      <td>0.047817</td>\n",
    "      <td>0.041849</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'common_voice_13_0_en_test'</th>\n",
    "      <td>1.030573</td>\n",
    "      <td>0.293778</td>\n",
    "      <td>0.288143</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'MathSpeech'</th>\n",
    "      <td>1.051637</td>\n",
    "      <td>0.341368</td>\n",
    "      <td>0.341368</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "### **parakeet-tdt-0.6b-v2**:\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>WER without preprocessing</th>\n",
    "      <th>WER with type A preprocessing</th>\n",
    "      <th>WER with type B preprocessing</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>'tedlium_3_test_segments'</th>\n",
    "      <td>0.406494</td>\n",
    "      <td>0.218562</td>\n",
    "      <td>0.176007</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_clean'</th>\n",
    "      <td>0.978238</td>\n",
    "      <td>0.013921</td>\n",
    "      <td>0.013879</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_other'</th>\n",
    "      <td>0.979126</td>\n",
    "      <td>0.039340</td>\n",
    "      <td>0.032721</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'common_voice_13_0_en_test'</th>\n",
    "      <td>0.193538</td>\n",
    "      <td>0.128687</td>\n",
    "      <td>0.121887</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'MathSpeech'</th>\n",
    "      <td>0.292922</td>\n",
    "      <td>0.118508</td>\n",
    "      <td>0.121297</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "### **parakeet-tdt-1.1b**:\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>WER without preprocessing</th>\n",
    "      <th>WER with type A preprocessing</th>\n",
    "      <th>WER with type B preprocessing</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>'tedlium_3_test_segments'</th>\n",
    "      <td>0.215337</td>\n",
    "      <td>0.215337</td>\n",
    "      <td>0.172384</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_clean'</th>\n",
    "      <td>1.001905</td>\n",
    "      <td>0.014003</td>\n",
    "      <td>0.013061</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_other'</th>\n",
    "      <td>1.000312</td>\n",
    "      <td>0.034371</td>\n",
    "      <td>0.027682</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'common_voice_13_0_en_test'</th>\n",
    "      <td>0.392436</td>\n",
    "      <td>0.102604</td>\n",
    "      <td>0.102109</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'MathSpeech'</th>\n",
    "      <td>0.227052</td>\n",
    "      <td>0.216707</td>\n",
    "      <td>0.220373</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "### **parakeet-rnnt-1.1b**:\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>WER without preprocessing</th>\n",
    "      <th>WER with type A preprocessing</th>\n",
    "      <th>WER with type B preprocessing</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>'tedlium_3_test_segments'</th>\n",
    "      <td>0.216413</td>\n",
    "      <td>0.216413</td>\n",
    "      <td>0.174144</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_clean'</th>\n",
    "      <td>1.001905</td>\n",
    "      <td>0.015180</td>\n",
    "      <td>0.013923</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_other'</th>\n",
    "      <td>1.001860</td>\n",
    "      <td>0.034105</td>\n",
    "      <td>0.025794</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'common_voice_13_0_en_test'</th>\n",
    "      <td>0.389216</td>\n",
    "      <td>0.094002</td>\n",
    "      <td>0.092888</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'MathSpeech'</th>\n",
    "      <td>0.240727</td>\n",
    "      <td>0.230382</td>\n",
    "      <td>0.234048</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "### **parakeet-ctc-0.6b**:\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>WER without preprocessing</th>\n",
    "      <th>WER with type A preprocessing</th>\n",
    "      <th>WER with type B preprocessing</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>'tedlium_3_test_segments'</th>\n",
    "      <td>0.211909</td>\n",
    "      <td>0.211909</td>\n",
    "      <td>0.169717</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_clean'</th>\n",
    "      <td>1.001905</td>\n",
    "      <td>0.020159</td>\n",
    "      <td>0.019471</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_other'</th>\n",
    "      <td>1.001339</td>\n",
    "      <td>0.058955</td>\n",
    "      <td>0.051514</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'common_voice_13_0_en_test'</th>\n",
    "      <td>0.439564</td>\n",
    "      <td>0.178469</td>\n",
    "      <td>0.174057</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'MathSpeech'</th>\n",
    "      <td>0.232928</td>\n",
    "      <td>0.218615</td>\n",
    "      <td>0.222293</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "### **parakeet-ctc-1.1b**:\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>WER without preprocessing</th>\n",
    "      <th>WER with type A preprocessing</th>\n",
    "      <th>WER with type B preprocessing</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>'tedlium_3_test_segments'</th>\n",
    "      <td>0.213947</td>\n",
    "      <td>0.213947</td>\n",
    "      <td>0.171533</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_clean'</th>\n",
    "      <td>1.003260</td>\n",
    "      <td>0.018914</td>\n",
    "      <td>0.016592</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'librispeech_test_other'</th>\n",
    "      <td>1.001884</td>\n",
    "      <td>0.052315</td>\n",
    "      <td>0.046274</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'common_voice_13_0_en_test'</th>\n",
    "      <td>0.416557</td>\n",
    "      <td>0.142409</td>\n",
    "      <td>0.140728</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>'MathSpeech'</th>\n",
    "      <td>0.255524</td>\n",
    "      <td>0.244322</td>\n",
    "      <td>0.247666</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "### The following table represents the comparsion between the time took each model to finish the test with the time took the \"parakeet-ctc-0.6b\" to do so. The reason why I chose this model is because it was the fastest.\n",
    "\n",
    "| Model | Duration Factor |\n",
    "|----------|:----------:|\n",
    "| whisper-tiny | 2.41935x |\n",
    "| whisper-base | 3.06452x |\n",
    "| whisper-small | 5.48387x |\n",
    "| whisper-medium | 11.93548x |\n",
    "| whisper-turbo | 9.32258x |\n",
    "| whisper-large | 19.83871x |\n",
    "| wav2vec2-base | 12.48387x |\n",
    "| wav2vec2-base-960h | 11.80645x |\n",
    "| wav2vec2-large-960h | 28.32258x |\n",
    "| wav2vec2-large-960h-lv60-self | 31.58065x |\n",
    "| parakeet-tdt-0.6b-v2 | 1.06452x |\n",
    "| parakeet-tdt-1.1b | 2.19355x |\n",
    "| parakeet-rnnt-1.1b | 1.90323x |\n",
    "| parakeet-ctc-0.6b | 1x |\n",
    "| parakeet-ctc-1.1b | 1.67742x |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0813860",
   "metadata": {},
   "source": [
    "## **8Ô∏è‚É£ Final Words** <a id=\"final-words\"></a>\n",
    "\n",
    "### Most models performed good at the tests, and some of them was very fast compared to the others. Since the models performances weren't bad, we might not need to fine-tune them. A lot of similar performance models, so making a solid decision now is a bit difficult. For now, these results will affect the decisions made in the future, but we won't take any solid decision right now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
