{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lpi5stpr7VFP"
      },
      "source": [
        "## **0️⃣ Table Of Contents**\n",
        "1. [Install & Download](#install-download)  \n",
        "2. [Initialize Models](#initialize-models)  \n",
        "3. [Pipeline](#pipeline)  \n",
        "4. [Backend](#backend)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrowAbTj7gkB"
      },
      "source": [
        "## **1️⃣ Install & Download** <a id=\"install-download\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "65sq7rvJ6gGU"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install -U yt-dlp\n",
        "!pip install -U openai-whisper\n",
        "!pip install scenedetect\n",
        "!pip install -q transformers accelerate pillow\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -U transformers\n",
        "!pip install accelerate\n",
        "!pip install nbformat\n",
        "!pip install sentence_transformers\n",
        "!pip install huggingface_hub[hf_xet]\n",
        "!pip install flask flask-ngrok\n",
        "!pip install pyngrok flask\n",
        "!pip install flask-cors\n",
        "!pip install flask flask-ngrok\n",
        "!pip install pyngrok flask\n",
        "!pip install flask-cors\n",
        "!pip install mysql-connector-python\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hv9eTGXU7dfB"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "import whisper\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "from huggingface_hub import login\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "import torch\n",
        "import json\n",
        "import re\n",
        "import datetime\n",
        "from typing import List, Dict, Any, Protocol\n",
        "from tqdm import tqdm\n",
        "import nbformat\n",
        "from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "!export TRANSFORMERS_VERBOSITY=error\n",
        "import copy\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from pyngrok import ngrok\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from flask import Flask, send_file, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "import time, threading\n",
        "import mysql.connector\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2fn7Dxg8t2Y"
      },
      "source": [
        "## **2️⃣ Initialize Models** <a id=\"initialize-models\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ77h8cR8tOo"
      },
      "outputs": [],
      "source": [
        "jf_token = '' # write you hf token\n",
        "\n",
        "login(jf_token) # Sometimes doesn't work for some reason. Thus, I also pass the token whenever I need to download anything from hf\n",
        "\n",
        "asr_model = whisper.load_model(\"base\").to(\"cuda\")\n",
        "\n",
        "vlm_model_id = \"HuggingFaceTB/SmolVLM-256M-Instruct\"\n",
        "vlm_processor = AutoProcessor.from_pretrained(vlm_model_id, use_auth_token=jf_token)\n",
        "vlm_model = AutoModelForVision2Seq.from_pretrained(vlm_model_id, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token=jf_token)\n",
        "\n",
        "class LLMModelAPI(Protocol):\n",
        "    def __init__(self, model_name):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_auth_token=jf_token)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            #load_in_4bit=True,\n",
        "            torch_dtype=torch.float16,\n",
        "            use_auth_token=jf_token\n",
        "        )\n",
        "        self.device = next(self.model.parameters()).device\n",
        "\n",
        "    def generate_text(self, prompt: str, max_new_tokens:int = 512, do_sample:bool=False) -> str:\n",
        "        enc = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "        gen_kwargs = dict(\n",
        "            input_ids=enc[\"input_ids\"],\n",
        "            attention_mask=enc.get(\"attention_mask\"),\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=do_sample,\n",
        "            eos_token_id=self.tokenizer.eos_token_id,\n",
        "            pad_token_id=self.tokenizer.pad_token_id,\n",
        "        )\n",
        "        out = self.model.generate(**gen_kwargs)\n",
        "        text = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "        if text.startswith(prompt):\n",
        "            text = text[len(prompt):].strip()\n",
        "        return text\n",
        "\n",
        "    def close(self):\n",
        "        del self.model\n",
        "\n",
        "llm_model = LLMModelAPI(\"Qwen/Qwen2.5-14B-Instruct-1M\")\n",
        "#llm_model = LLMModelAPI(\"Qwen/Qwen2.5-7B-Instruct-1M\")\n",
        "\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\", token=jf_token)\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6Vy5n4r7d31"
      },
      "source": [
        "## **3️⃣ Pipeline** <a id=\"pipeline\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PInIRYBj8BAO"
      },
      "outputs": [],
      "source": [
        "def download_video(url):\n",
        "  !rm -rf downloaded\n",
        "\n",
        "  # download full video for the VLM model\n",
        "  video_result = subprocess.run([\n",
        "      'yt-dlp',\n",
        "      '-f', 'best[ext=mp4]',\n",
        "      '-o', os.path.join('downloaded', 'video.mp4'),\n",
        "      url.split('&')[0]\n",
        "  ])\n",
        "\n",
        "  # download audio only for the ASR model\n",
        "  audio_result = subprocess.run([\n",
        "      'yt-dlp',\n",
        "      '-x', '--audio-format', 'mp3',\n",
        "      '-o', os.path.join('downloaded', 'audio.mp3'),\n",
        "      '--postprocessor-args', '-ac 1',# mono\n",
        "      url.split('&')[0]\n",
        "  ])\n",
        "\n",
        "  return video_result.returncode == 0 and audio_result.returncode == 0\n",
        "\n",
        "def transcript_extraction():\n",
        "  \"\"\"\n",
        "    This method converts the audio file into transcripts and return a list of\n",
        "    segments, similar to the following format:\n",
        "        [\n",
        "          {\n",
        "            'start' : 0.5,\n",
        "            'end' : 2.7,\n",
        "            'text' : 'LMH is a beautiful place.'\n",
        "          },\n",
        "          {\n",
        "            'start' : 3.3,\n",
        "            'end' : 5,\n",
        "            'text' : 'It is filled with interesting activities.'\n",
        "          }\n",
        "        ]\n",
        "  \"\"\"\n",
        "  return asr_model.transcribe(\"/content/downloaded/audio.mp3\", word_timestamps=True)['segments']\n",
        "\n",
        "def keyframes_extraction():\n",
        "  \"\"\"\n",
        "    This method extracts the keyframes of the downloaded video, and returns a list as follows:\n",
        "        [\n",
        "          {'time': 0.0, 'image_path': 'keyframes/video-Scene-001-03.jpg'},\n",
        "          {'time': 7.875, 'image_path': 'keyframes/video-Scene-002-02.jpg'},\n",
        "          {'time': 19.987, 'image_path': 'keyframes/video-Scene-003-02.jpg'}\n",
        "        ]\n",
        "  \"\"\"\n",
        "  # delete the previous keyframes\n",
        "  !rm -rf keyframes\n",
        "\n",
        "  # extract the keyframes\n",
        "  !scenedetect -i \"/content/downloaded/video.mp4\" detect-content list-scenes --output keyframes save-images --output keyframes\n",
        "\n",
        "  df = pd.read_csv(\"keyframes/video-Scenes.csv\", skiprows=1)\n",
        "  keyframes = []\n",
        "\n",
        "  # attach an approximate time for each keyframe\n",
        "  for _, row in df.iterrows():\n",
        "      current_scene = row['Scene Number'] # the number of the current scene\n",
        "      start = row['Start Time (seconds)'] # the beginning time of the current scene\n",
        "      length = row['Length (seconds)'] # the duration of the current scene\n",
        "      frames = glob(f'keyframes/video-Scene-{current_scene:03}-*') # list of the current scene's keyframes\n",
        "      for i, frame in enumerate(frames):\n",
        "          keyframes.append({\n",
        "              # this is the linear interpolation method (LERP)\n",
        "              # it's simply divides the duration between all the current scene's keyframes linearly\n",
        "              # for example, let:\n",
        "              #     start = 1.2\n",
        "              #     length = 5.8\n",
        "              #     len(frames) = 5\n",
        "              # then, the time of each keyframe is: 1.2, 2.65, 4.1, 5.55, 7, respectfully\n",
        "              'time' : start + length * (i / (len(frames) - 1) if len(frames) > 1 else 1),\n",
        "              'image_path' : frame\n",
        "          })\n",
        "          break # for simplicity, I am only taking the first keyframe of each scene\n",
        "\n",
        "  return keyframes\n",
        "\n",
        "def chunkig(segments, keyframes):\n",
        "  \"\"\"\n",
        "    This method takes inputs from both the \"transcript_extraction()\" and \"keyframes_extraction()\" methods\n",
        "    and attach each keyframe to a group of text segments that were mentioned in a cloose time period\n",
        "    the output is as follows:\n",
        "        [\n",
        "          {\n",
        "            'start': np.float64(0.0),\n",
        "            'end': np.float64(5.14),\n",
        "            'speech': ' Python, a high-level, interpreted programming language famous for its zen-like code.',\n",
        "            'keyframe_path': 'keyframes/video-Scene-001-03.jpg'\n",
        "          },\n",
        "          {\n",
        "            'start': np.float64(5.32),\n",
        "            'end': np.float64(14.44),\n",
        "            'speech': \" It's arguably the most popular language in the world because it's easy to learn,  yet practical for serious projects. In fact, you're watching this YouTube video in a Python web\",\n",
        "            'keyframe_path': 'keyframes/video-Scene-002-02.jpg'\n",
        "          }\n",
        "        ]\n",
        "  \"\"\"\n",
        "\n",
        "  # if there is only one keyframe, return one chunk\n",
        "  if len(keyframes) == 1:\n",
        "      full_speech = \" \".join(s['text'] for s in segments)\n",
        "      return [{\n",
        "          \"start\": segments[0]['start'],\n",
        "          \"end\": segments[-1]['end'],\n",
        "          \"speech\": full_speech,\n",
        "          \"keyframe_path\": keyframes[0]['image_path']\n",
        "      }]\n",
        "\n",
        "  # create a list for each keyframe to hold its corresponding text segments\n",
        "  chunk_per_keyframe = [[] for _ in keyframes]\n",
        "  for segment in segments:\n",
        "    segment_midpoint = (segment['start'] + segment['end']) / 2.0\n",
        "\n",
        "    distances = [abs(segment_midpoint - kf['time']) for kf in keyframes]\n",
        "    closest_keyframe_idx = distances.index(min(distances))\n",
        "\n",
        "    chunk_per_keyframe[closest_keyframe_idx].append(segment)\n",
        "\n",
        "  # merge the text segments that corresponds to a similar keyframe together\n",
        "  final_chunks = []\n",
        "  for i, segments in enumerate(chunk_per_keyframe):\n",
        "    if not segments:\n",
        "        continue\n",
        "    segments.sort(key=lambda s: s['start'])\n",
        "\n",
        "    chunk_speech = \" \".join(s['text'] for s in segments)\n",
        "\n",
        "    final_chunks.append({\n",
        "        \"start\": segments[0]['start'],\n",
        "        \"end\": segments[-1]['end'],\n",
        "        \"speech\": chunk_speech,\n",
        "        \"keyframe_path\": keyframes[i]['image_path']\n",
        "    })\n",
        "\n",
        "  return final_chunks\n",
        "\n",
        "def visual_information_extraction(chunks):\n",
        "  \"\"\"\n",
        "    This method takes the list of chunks produced by the \"chunkig(segments, keyframes)\" method\n",
        "    as input and returns a version of chunks that contain a text description for each keyframe.\n",
        "    the output format is as follows:\n",
        "        [\n",
        "          {\n",
        "            'start': np.float64(0.0),\n",
        "            'end': np.float64(5.14),\n",
        "            'speech': ' Python, a high-level, interpreted programming language famous for its zen-like code.',\n",
        "            'keyframe_path': 'keyframes/video-Scene-001-03.jpg',\n",
        "            'keyframe_description': 'The visual content in this keyframe is a VIBE (Visual Basic) learning chart. The chart is divided into five sections, each representing a different language. The chart is titled \"ViB\" and is labeled as \"ViB\" in the bottom right corner.\\n\\nThe chart is divided into five sections:\\n1. Python\\n2. C\\n3. Java\\n4. C++\\n5. Visual Basic\\n\\nEach section is labeled with a number and a percentage value. The percentages are:\\n- 11.27% for Python\\n- 10.46% for C\\n- 10.16% for C++\\n- 10.04% for Java\\n- 10.02% for C++\\n- 10.01% for Visual Basic\\n\\nThe chart is designed to help learners understand the differences between different programming languages. The percentages indicate the percentage of each language being'\n",
        "          },\n",
        "          {\n",
        "            'start': np.float64(5.32),\n",
        "            'end': np.float64(14.44),\n",
        "            'speech': \" It's arguably the most popular language in the world because it's easy to learn,  yet practical for serious projects. In fact, you're watching this YouTube video in a Python web\",\n",
        "            'keyframe_path': 'keyframes/video-Scene-002-02.jpg',\n",
        "            'keyframe_description': 'The keyframe shows a monitor screen displaying a sequence of numbers and letters. The sequence includes the numbers 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 4'\n",
        "          }\n",
        "        ]\n",
        "  \"\"\"\n",
        "  # Loop through keyframes and get description\n",
        "  for item in chunks:\n",
        "    img = Image.open(item[\"keyframe_path\"]).convert(\"RGB\")\n",
        "\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": img},\n",
        "            {\"type\": \"text\", \"text\": \"Describe the visual content in this educational keyframe. Mention diagrams, equations, and the main topic.\"}\n",
        "        ],\n",
        "    }]\n",
        "\n",
        "    text = vlm_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = vlm_processor(text=[text], images=[img], return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "    out = vlm_model.generate(**inputs, max_new_tokens=200)\n",
        "    desc = vlm_processor.batch_decode(out, skip_special_tokens=True)[0].split(\"Assistant:\", 1)[1].strip()\n",
        "    item[\"keyframe_description\"] = desc\n",
        "\n",
        "  return chunks\n",
        "\n",
        "def rewrite_transcription(_chunks):\n",
        "  \"\"\"\n",
        "    just rewrites the transcripts using the llm model for better quality.\n",
        "    this method takes the input from the method: \"visual_information_extraction(chunks)\",\n",
        "    and return an output with the exact same format.\n",
        "  \"\"\"\n",
        "  chunks = copy.deepcopy(_chunks)\n",
        "  rewrite_prompt = f'You are given a text transcription from a lecture video, clean it if needed and reorganize it. Do not add anything extra, just the clearified text. The transcription: \"<TEXT>\".\\n\\n Your clearified version:'\n",
        "  for i in range(len(chunks)):\n",
        "      prompt = rewrite_prompt.replace(\"<TEXT>\", chunks[i][\"speech\"])\n",
        "      rewritten = llm_model.generate_text(prompt)\n",
        "      chunks[i][\"speech\"] = rewritten\n",
        "  return chunks\n",
        "\n",
        "def blend(_chunks):\n",
        "  \"\"\"\n",
        "    this method combined/blends the visual information and the transcriptions together using the llm model.\n",
        "    it takes the input from the method: \"rewrite_transcription(_chunks)\", and return a list as follows:\n",
        "        [\n",
        "          {\n",
        "            'start': np.float64(0.0),\n",
        "            'end': np.float64(5.14),\n",
        "            'speech': 'Python is a high-level, interpreted programming language known for its clean and elegant code.',\n",
        "            'keyframe_path': 'keyframes/video-Scene-001-03.jpg',\n",
        "            'keyframe_description': 'The visual content in this keyframe is a VIBE (Visual Basic) learning chart. The chart is divided into five sections, each representing a different language. The chart is titled \"ViB\" and is labeled as \"ViB\" in the bottom right corner.\\n\\nThe chart is divided into five sections:\\n1. Python\\n2. C\\n3. Java\\n4. C++\\n5. Visual Basic\\n\\nEach section is labeled with a number and a percentage value. The percentages are:\\n- 11.27% for Python\\n- 10.46% for C\\n- 10.16% for C++\\n- 10.04% for Java\\n- 10.02% for C++\\n- 10.01% for Visual Basic\\n\\nThe chart is designed to help learners understand the differences between different programming languages. The percentages indicate the percentage of each language being',\n",
        "            'blended_text': '\"Python is a high-level, interpreted programming language known for its clean and elegant code. In the context of the VIBE learning chart displayed, Python accounts for 11.27% of the total representation, highlighting its significance among other languages like C, Java, C++, and Visual Basic.\"'\n",
        "          },\n",
        "          {'start': np.float64(5.32),\n",
        "            'end': np.float64(14.44),\n",
        "            'speech': '\"It is arguably the most popular language in the world because it is easy to learn yet practical for serious projects. In fact, you are watching this YouTube video on a Python web platform.\"',\n",
        "            'keyframe_path': 'keyframes/video-Scene-002-02.jpg',\n",
        "            'keyframe_description': 'The keyframe shows a monitor screen displaying a sequence of numbers and letters. The sequence includes the numbers 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 4',\n",
        "            'blended_text': '\"It is arguably the most popular language in the world because it is easy to learn yet practical for serious projects. In fact, you are watching this YouTube video on a Python web platform, which is displayed on the monitor showing a sequence of numbers and letters.\"'\n",
        "          }\n",
        "        ]\n",
        "  \"\"\"\n",
        "  chunks = copy.deepcopy(_chunks)\n",
        "  blend_prompt = f'You are given a text transcription from a lecture video, alongside the visual description of the lecture, your task is to blend both of these texts and generate a useful new text that emphasizes both the transcription and the visual description. Do not add anything extra, just the clearified text. The transcription: \"<TRANSCRIPTION>\".\\n\\n The visual descriptoin: \"<VISUAL_DESCRIPTION>\".\\n\\n The blended version: '\n",
        "  for i in range(len(chunks)):\n",
        "      prompt = blend_prompt.replace(\"<TRANSCRIPTION>\", chunks[i][\"speech\"]).replace(\"<VISUAL_DESCRIPTION>\", chunks[i][\"keyframe_description\"])\n",
        "      blended = llm_model.generate_text(prompt)\n",
        "      chunks[i][\"blended_text\"] = blended\n",
        "  return chunks\n",
        "\n",
        "def segment_blended_text(obj, alpha=0.6):\n",
        "  blended_texts = [obj[i][\"blended_text\"] for i in range(len(obj))]\n",
        "  embeddings = embedder.encode(blended_texts, convert_to_tensor=True)\n",
        "  segments = []\n",
        "  current_segment = [blended_texts[0]]\n",
        "\n",
        "  for i in range(1, len(blended_texts)):\n",
        "      sim = util.cos_sim(embeddings[i], embeddings[i-1]).item()\n",
        "      if sim > alpha:\n",
        "          current_segment.append(blended_texts[i])\n",
        "      else:\n",
        "          segments.append(current_segment)\n",
        "          current_segment = [blended_texts[i]]\n",
        "\n",
        "  segments.append(current_segment)\n",
        "\n",
        "  return [' '.join(joined) for joined in segments]\n",
        "\n",
        "def write_notebook(segments, notebook_name):\n",
        "  INTRO_PROMPT = r\"\"\"\n",
        "  Write the introduction part of a jupyter notebook given the following text:\n",
        "\n",
        "  <SEGMENT>\n",
        "  {segment_text}\n",
        "  </SEGMENT>\n",
        "\n",
        "  Task:\n",
        "  1) Produce **rich markdown** cells describing:\n",
        "    - What this notebook is about (summary/purpose).\n",
        "    - Who it's for and prerequisites.\n",
        "    - Key learning outcomes (bulleted).\n",
        "    - A short suggested roadmap (2-4 items).\n",
        "  2) **Always** write formulas, if there are any, in the correct markdown format using $$.\n",
        "  3) Use headers to splits topics.\n",
        "  4) Don't write too much, stop when needed.\n",
        "  5) If you wrote summary, called it abstract instead.\n",
        "  6) Each markdown cell should be written like the following:\n",
        "  start+++markdown\n",
        "  contents...\n",
        "  end+++markdown\n",
        "\n",
        "  Your response:\n",
        "  \"\"\"\n",
        "\n",
        "  SEGMENT_PROMPT = r\"\"\"\n",
        "  <FINISHED PART>\n",
        "  {finished_part}\n",
        "  </FINISHED PART>\n",
        "\n",
        "  Above the finished parts of a jupyter notebook. Complete the notebook by writing cells that explain the following text segment in details with examples and quizes and using formulas, all as needed:\n",
        "\n",
        "  <SEGMENT>\n",
        "  {segment_text}\n",
        "  </SEGMENT>\n",
        "\n",
        "  Task:\n",
        "  1) Produce **markdown** or **code** cells describing everything was mentioned in this segment of text.\n",
        "  2) **Always** write formulas, if there are any, in the correct markdown format using $$.\n",
        "  3) Use headers to splits topics.\n",
        "  4) Write codes if needed.\n",
        "  5) Don't write too much, stop when needed.\n",
        "  6) Only add more to the notebook, but don't repeat what was previously mentioned and explained.\n",
        "  7) Do not write summaries at all.\n",
        "  8) Each markdown cell should be written like the following:\n",
        "  start+++markdown\n",
        "  contents...\n",
        "  end+++markdown\n",
        "\n",
        "  Your response:\n",
        "  \"\"\"\n",
        "\n",
        "  EXTRO_PROMPT = r\"\"\"\n",
        "  <FINISHED PART>\n",
        "  {finished_part}\n",
        "  </FINISHED PART>\n",
        "\n",
        "  Given the above finished part of a jupyter notebook, produce cells for the conclusion part that summarize all the previous parts.\n",
        "\n",
        "  Task:\n",
        "  1) **Always** write formulas, if there are any, in the correct markdown format using $$.\n",
        "  2) Use headers to splits topics.\n",
        "  3) Don't write too much, stop when needed.\n",
        "  4) Each markdown cell should be written like the following:\n",
        "  start+++markdown\n",
        "  contents...\n",
        "  end+++markdown\n",
        "\n",
        "  Your response:\n",
        "  \"\"\"\n",
        "\n",
        "  # create a notebook object\n",
        "  nb = new_notebook()\n",
        "  nb[\"metadata\"] = {\n",
        "      \"created_by\": \"The best project ever!\",\n",
        "      \"created_at\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
        "  }\n",
        "  nb[\"cells\"] = []\n",
        "\n",
        "  # iterate through all the segments and combined them to create the introductory section\n",
        "  excerpts = []\n",
        "  for seg in tqdm(segments, leave=False):\n",
        "      excerpts.append(seg.strip())\n",
        "  joined = \"\\n\\n---\\n\\n\".join(excerpts)\n",
        "  prompt = INTRO_PROMPT.format(segment_text=joined)\n",
        "\n",
        "  # this is the introduction\n",
        "  finished = llm_model.generate_text(prompt, max_new_tokens=4096, do_sample=True)\n",
        "\n",
        "  # add cells to the notebook object\n",
        "  # each generated cell is encapsulated in start+++markdown ... end+++markdown or start+++code ... end+++code, so the\n",
        "  # following code extract all the cells, and iterate through them\n",
        "  for i, (t, cell) in enumerate(re.findall(r\"start\\+\\+\\+(markdown|code)\\s*(.*?)end\\+\\+\\+[markdown|code]\", finished, re.DOTALL), 1):\n",
        "    # add cell with the correct type (code or markdown)\n",
        "    nb[\"cells\"].append(new_markdown_cell(cell) if t == 'markdown' else new_code_cell(cell))\n",
        "\n",
        "  # create the notebook part for each segment\n",
        "  for seg in tqdm(segments, leave=False):\n",
        "    # the generated segment\n",
        "    out = llm_model.generate_text(SEGMENT_PROMPT.format(segment_text=seg.strip(), finished_part=finished.strip()), max_new_tokens=4096, do_sample=True)\n",
        "    finished += '\\n' + out\n",
        "    # each generated cell is encapsulated in start+++markdown ... end+++markdown or start+++code ... end+++code, so the\n",
        "    # following code extract all the cells, and iterate through them\n",
        "    for i, (t, cell) in enumerate(re.findall(r\"start\\+\\+\\+(markdown|code)\\s*(.*?)end\\+\\+\\+[markdown|code]\", out, re.DOTALL), 1):\n",
        "        # add cell with the correct type (code or markdown)\n",
        "        nb[\"cells\"].append(new_markdown_cell(cell) if t == 'markdown' else new_code_cell(cell))\n",
        "\n",
        "  extro = llm_model.generate_text(EXTRO_PROMPT.format(finished_part=finished.strip()), max_new_tokens=4096, do_sample=True)\n",
        "  for i, (t, cell) in enumerate(re.findall(r\"start\\+\\+\\+(markdown|code)\\s*(.*?)end\\+\\+\\+[markdown|code]\", extro, re.DOTALL), 1):\n",
        "    nb[\"cells\"].append(new_markdown_cell(cell) if t == 'markdown' else new_code_cell(cell))\n",
        "\n",
        "  # save the generated notebook at disk\n",
        "  nbformat.write(nb, notebook_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQlUFhLGJuh9"
      },
      "source": [
        "## **4️⃣ Backend** <a id=\"backend\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-_cm7ewVg7IT"
      },
      "outputs": [],
      "source": [
        "ngrok.set_auth_token(\"\") # Write your auth token\n",
        "\n",
        "# \"status\": \"running\", \"ready\", or \"idle\"\n",
        "progress = {\"status\": \"idle\", \"percentage\": \"0.0%\", 'details' : 'downloading the video...'}\n",
        "notebook_name = ''\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "@app.route(\"/process\", methods=[\"POST\"])\n",
        "def process():\n",
        "    global progress\n",
        "    if progress[\"status\"] != 'idle':\n",
        "        return jsonify({\"message\": \"The server is busy!\"})\n",
        "\n",
        "    data = request.json\n",
        "    user_input = data.get(\"input\", \"\")\n",
        "    global notebook_name\n",
        "    notebook_name = user_input.split('v=')[1]  + '.ipynb'\n",
        "\n",
        "    def background_job(url, notebook_name):\n",
        "        global progress\n",
        "        try:\n",
        "            progress[\"status\"] = 'running'\n",
        "\n",
        "            progress[\"percentage\"] = '0.0%'\n",
        "            progress[\"details\"] = 'checking the cash...'\n",
        "            conn = mysql.connector.connect(\n",
        "            host=\"srv1798.hstgr.io\",\n",
        "            user=\"u370625083_v2n\",\n",
        "            password=\"v2n_ABCD\",\n",
        "            database=\"u370625083_v2n\"\n",
        "            )\n",
        "            cursor = conn.cursor()\n",
        "            sql = \"SELECT id FROM cashed_notebooks WHERE url = %s\"\n",
        "            cursor.execute(sql, (url,))\n",
        "            repeated = cursor.fetchone()\n",
        "            cursor.close()\n",
        "            conn.close()\n",
        "\n",
        "            if repeated:\n",
        "                progress[\"percentage\"] = '50%'\n",
        "                progress[\"details\"] = 'downloading cash...'\n",
        "                conn = mysql.connector.connect(\n",
        "                host=\"srv1798.hstgr.io\",\n",
        "                user=\"u370625083_v2n\",\n",
        "                password=\"v2n_ABCD\",\n",
        "                database=\"u370625083_v2n\"\n",
        "                )\n",
        "                cursor = conn.cursor()\n",
        "                sql = \"SELECT notebook FROM cashed_notebooks WHERE url = %s\"\n",
        "                cursor.execute(sql, (url,))\n",
        "                result = cursor.fetchone()\n",
        "                with open(notebook_name, \"wb\") as f:\n",
        "                    f.write(result[0])\n",
        "                cursor.close()\n",
        "                conn.close()\n",
        "                progress[\"percentage\"] = '100%'\n",
        "                progress[\"details\"] = 'finished!'\n",
        "                progress[\"status\"] = \"ready\"\n",
        "                return\n",
        "\n",
        "            progress[\"percentage\"] = '9%'\n",
        "            progress[\"details\"] = 'downloading the video...'\n",
        "            download_status = download_video(url)\n",
        "            if not download_status:\n",
        "              print('can\\'t download the video!')\n",
        "              progress[\"status\"] = \"idle\"\n",
        "              return\n",
        "\n",
        "            progress[\"percentage\"] = '18%'\n",
        "            progress[\"details\"] = 'extracting transcripts...'\n",
        "            segments = transcript_extraction()\n",
        "\n",
        "            progress[\"percentage\"] = '27%'\n",
        "            progress[\"details\"] = 'extracting keyframes...'\n",
        "            keyframes = keyframes_extraction()\n",
        "\n",
        "            progress[\"percentage\"] = '36%'\n",
        "            progress[\"details\"] = 'chunkig...'\n",
        "            segments = chunkig(segments, keyframes)\n",
        "\n",
        "            progress[\"percentage\"] = '45%'\n",
        "            progress[\"details\"] = 'extracting visual information...'\n",
        "            segments = visual_information_extraction(segments)\n",
        "\n",
        "            progress[\"percentage\"] = '54%'\n",
        "            progress[\"details\"] = 'rewriting transcriptions...'\n",
        "            segments = rewrite_transcription(segments)\n",
        "\n",
        "            progress[\"percentage\"] = '63%'\n",
        "            progress[\"details\"] = 'belnding...'\n",
        "            segments = blend(segments)\n",
        "\n",
        "            progress[\"percentage\"] = '72%'\n",
        "            progress[\"details\"] = 'resegmenting...'\n",
        "            segments = segment_blended_text(segments, alpha=0.6)\n",
        "\n",
        "            progress[\"percentage\"] = '81%'\n",
        "            progress[\"details\"] = 'generating notebook...'\n",
        "            write_notebook(segments, notebook_name)\n",
        "\n",
        "            if not repeated:\n",
        "                progress[\"percentage\"] = '90%'\n",
        "                progress[\"details\"] = 'cashing...'\n",
        "                conn = mysql.connector.connect(\n",
        "                host=\"srv1798.hstgr.io\",\n",
        "                user=\"u370625083_v2n\",\n",
        "                password=\"v2n_ABCD\",\n",
        "                database=\"u370625083_v2n\"\n",
        "                )\n",
        "                cursor = conn.cursor()\n",
        "\n",
        "                with open(notebook_name, \"rb\") as f:\n",
        "                    file_data = f.read()\n",
        "                sql = \"INSERT INTO cashed_notebooks (url, notebook) VALUES (%s, %s)\"\n",
        "                cursor.execute(sql, (url, file_data))\n",
        "                conn.commit()\n",
        "\n",
        "                cursor.close()\n",
        "                conn.close()\n",
        "\n",
        "            progress[\"percentage\"] = '100%'\n",
        "            progress[\"details\"] = 'finished!'\n",
        "            progress[\"status\"] = \"ready\"\n",
        "\n",
        "        except Exception as ex:\n",
        "          progress[\"status\"] = \"idle\"\n",
        "          print(f'ex:\\n{ex.message}')\n",
        "\n",
        "    # Run job in separate thread\n",
        "    threading.Thread(target=background_job, args=(user_input, notebook_name)).start()\n",
        "\n",
        "    return jsonify({\"message\": \"Job started!\"})\n",
        "\n",
        "@app.route(\"/status\")\n",
        "def get_status():\n",
        "    return jsonify(progress)\n",
        "\n",
        "@app.route(\"/download\")\n",
        "def download():\n",
        "    time.sleep(1)\n",
        "    global progress\n",
        "    progress[\"status\"] = 'idle'\n",
        "    global notebook_name\n",
        "    return send_file(notebook_name, as_attachment=True)\n",
        "\n",
        "public_url = ngrok.connect(5000, domain='') # write your public domain\n",
        "print(\"Public URL:\", public_url)\n",
        "app.run(port=5000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
